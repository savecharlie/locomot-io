{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LOCOMOT.IO Self-Play Training\n",
    "\n",
    "Train the neural network AI by playing against itself. Creates a curriculum of progressively harder opponents.\n",
    "\n",
    "**Architecture:**\n",
    "- Input: 48 values (8 directions × 6 features)\n",
    "- Hidden: 64 → 64 (ReLU)\n",
    "- Output: 3 (left, straight, right)\n",
    "\n",
    "**Self-Play Strategy:**\n",
    "1. Start with current trained model\n",
    "2. Play games against copies of itself\n",
    "3. Learn from wins and losses\n",
    "4. Keep a pool of past versions for diverse training"
   ],
   "metadata": {
    "id": "intro"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network Architecture\n",
    "Matches the JavaScript implementation exactly"
   ],
   "metadata": {
    "id": "nn_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class LocomotNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(48, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def get_action(self, state, epsilon=0.0):\n",
    "        \"\"\"Get action with epsilon-greedy exploration\"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, 2)\n",
    "        with torch.no_grad():\n",
    "            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = self.forward(state_t)\n",
    "            return q_values.argmax(dim=1).item()\n",
    "\n",
    "def load_existing_weights(model, json_path):\n",
    "    \"\"\"Load weights from JavaScript brain.json format\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        weights = json.load(f)\n",
    "    \n",
    "    state_dict = {\n",
    "        'net.0.weight': torch.FloatTensor(weights['net.0.weight']),\n",
    "        'net.0.bias': torch.FloatTensor(weights['net.0.bias']),\n",
    "        'net.2.weight': torch.FloatTensor(weights['net.2.weight']),\n",
    "        'net.2.bias': torch.FloatTensor(weights['net.2.bias']),\n",
    "        'net.4.weight': torch.FloatTensor(weights['net.4.weight']),\n",
    "        'net.4.bias': torch.FloatTensor(weights['net.4.bias']),\n",
    "    }\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def save_brain_json(model, path):\n",
    "    \"\"\"Export weights to JavaScript-compatible format\"\"\"\n",
    "    state_dict = model.state_dict()\n",
    "    brain = {\n",
    "        'net.0.weight': state_dict['net.0.weight'].cpu().numpy().tolist(),\n",
    "        'net.0.bias': state_dict['net.0.bias'].cpu().numpy().tolist(),\n",
    "        'net.2.weight': state_dict['net.2.weight'].cpu().numpy().tolist(),\n",
    "        'net.2.bias': state_dict['net.2.bias'].cpu().numpy().tolist(),\n",
    "        'net.4.weight': state_dict['net.4.weight'].cpu().numpy().tolist(),\n",
    "        'net.4.bias': state_dict['net.4.bias'].cpu().numpy().tolist(),\n",
    "    }\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(brain, f)\n",
    "    print(f'Saved brain to {path}')"
   ],
   "metadata": {
    "id": "network"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Game Environment\n",
    "Simplified version of LOCOMOT.IO for fast training"
   ],
   "metadata": {
    "id": "env_header"
   }
  },
  {
   "cell_type": "code",
   "source": "class LocomotEnv:\n    \"\"\"FAST simplified LOCOMOT.IO environment for self-play\"\"\"\n    \n    WORLD_COLS = 50  # Smaller world = faster\n    WORLD_ROWS = 40\n    DIRECTIONS = [(0, -1), (1, 0), (0, 1), (-1, 0)]  # UP, RIGHT, DOWN, LEFT\n    \n    # Pre-compute direction offsets for 8 rays\n    RAY_OFFSETS = [\n        (0, -1), (1, -1), (1, 0), (1, 1),\n        (0, 1), (-1, 1), (-1, 0), (-1, -1)\n    ]\n    \n    def __init__(self, num_agents=4):\n        self.num_agents = num_agents\n        self.reset()\n    \n    def reset(self):\n        self.agents = []\n        self.pickup_set = set()  # Use set for O(1) lookup\n        self.segment_map = {}  # (x,y) -> (agent_idx, seg_idx) for fast collision\n        self.step_count = 0\n        \n        # Spawn agents\n        for i in range(self.num_agents):\n            x = random.randint(8, self.WORLD_COLS - 8)\n            y = random.randint(8, self.WORLD_ROWS - 8)\n            direction = random.randint(0, 3)\n            dx, dy = self.DIRECTIONS[direction]\n            segments = [(x - j*dx, y - j*dy) for j in range(4)]\n            \n            self.agents.append({\n                'segments': segments,\n                'direction': direction,\n                'alive': True,\n                'score': 0\n            })\n            \n            # Add to segment map\n            for seg_idx, seg in enumerate(segments):\n                self.segment_map[seg] = (i, seg_idx)\n        \n        # Spawn food\n        for _ in range(20):\n            self.pickup_set.add((\n                random.randint(0, self.WORLD_COLS - 1),\n                random.randint(0, self.WORLD_ROWS - 1)\n            ))\n        \n        return [self.get_vision_fast(i) for i in range(self.num_agents)]\n    \n    def get_vision_fast(self, agent_idx):\n        \"\"\"FAST vision using pre-computed offsets and set lookups\"\"\"\n        agent = self.agents[agent_idx]\n        if not agent['alive']:\n            return np.zeros(48, dtype=np.float32)\n        \n        head_x, head_y = agent['segments'][0]\n        current_dir = agent['direction']\n        my_length = len(agent['segments'])\n        my_segments = set(agent['segments'][1:])  # Skip head\n        \n        vision = np.zeros(48, dtype=np.float32)\n        \n        # Rotate ray offsets based on direction\n        # Direction 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n        rot = current_dir\n        \n        for ray_idx in range(8):\n            # Rotate ray index by current direction\n            rotated_idx = (ray_idx + rot * 2) % 8\n            dx, dy = self.RAY_OFFSETS[rotated_idx]\n            \n            food_dist = self_danger = wall_dist = 0.0\n            enemy_smaller = enemy_bigger = enemy_body = 0.0\n            \n            for dist in range(1, 16):  # Shorter rays = faster\n                cx, cy = head_x + dx * dist, head_y + dy * dist\n                \n                # Wall\n                if cx < 0 or cx >= self.WORLD_COLS or cy < 0 or cy >= self.WORLD_ROWS:\n                    if wall_dist == 0:\n                        wall_dist = 1.0 / dist\n                    break\n                \n                # Self body\n                if self_danger == 0 and (cx, cy) in my_segments:\n                    self_danger = 1.0 / dist\n                \n                # Food\n                if food_dist == 0 and (cx, cy) in self.pickup_set:\n                    food_dist = 1.0 / dist\n                \n                # Other agents (use segment map)\n                pos = (cx, cy)\n                if pos in self.segment_map:\n                    other_idx, seg_idx = self.segment_map[pos]\n                    if other_idx != agent_idx and self.agents[other_idx]['alive']:\n                        other_len = len(self.agents[other_idx]['segments'])\n                        if seg_idx == 0:  # Head\n                            if other_len < my_length and enemy_smaller == 0:\n                                enemy_smaller = 1.0 / dist\n                            elif enemy_bigger == 0:\n                                enemy_bigger = 1.0 / dist\n                        elif enemy_body == 0:\n                            enemy_body = 1.0 / dist\n            \n            base = ray_idx * 6\n            vision[base:base+6] = [food_dist, self_danger, wall_dist, enemy_smaller, enemy_bigger, enemy_body]\n        \n        return vision\n    \n    def step(self, actions):\n        \"\"\"Execute actions. Returns (observations, rewards, dones, game_over)\"\"\"\n        self.step_count += 1\n        rewards = np.zeros(self.num_agents, dtype=np.float32)\n        \n        # Apply turns\n        for i, agent in enumerate(self.agents):\n            if not agent['alive']:\n                continue\n            action = actions[i]\n            if action == 0:\n                agent['direction'] = (agent['direction'] - 1) % 4\n            elif action == 2:\n                agent['direction'] = (agent['direction'] + 1) % 4\n        \n        # Move agents and update segment map\n        new_heads = []\n        for i, agent in enumerate(self.agents):\n            if not agent['alive']:\n                new_heads.append(None)\n                continue\n            \n            # Remove old tail from map\n            old_tail = agent['segments'][-1]\n            if old_tail in self.segment_map and self.segment_map[old_tail][0] == i:\n                del self.segment_map[old_tail]\n            \n            dx, dy = self.DIRECTIONS[agent['direction']]\n            new_head = (agent['segments'][0][0] + dx, agent['segments'][0][1] + dy)\n            new_heads.append(new_head)\n            \n            agent['segments'].insert(0, new_head)\n            agent['segments'].pop()\n            \n            # Update segment map\n            for seg_idx, seg in enumerate(agent['segments']):\n                self.segment_map[seg] = (i, seg_idx)\n            \n            rewards[i] += 0.01\n        \n        # Check collisions\n        for i, agent in enumerate(self.agents):\n            if not agent['alive']:\n                continue\n            \n            hx, hy = agent['segments'][0]\n            \n            # Wall\n            if hx < 0 or hx >= self.WORLD_COLS or hy < 0 or hy >= self.WORLD_ROWS:\n                agent['alive'] = False\n                rewards[i] -= 5.0\n                continue\n            \n            # Self collision\n            if (hx, hy) in set(agent['segments'][1:]):\n                agent['alive'] = False\n                rewards[i] -= 5.0\n                continue\n            \n            # Enemy collision\n            for j, other in enumerate(self.agents):\n                if i == j or not other['alive']:\n                    continue\n                \n                oh = other['segments'][0]\n                if hx == oh[0] and hy == oh[1]:\n                    # Head-to-head\n                    if len(agent['segments']) > len(other['segments']):\n                        other['alive'] = False\n                        rewards[j] -= 5.0\n                        rewards[i] += 3.0\n                    elif len(agent['segments']) < len(other['segments']):\n                        agent['alive'] = False\n                        rewards[i] -= 5.0\n                        rewards[j] += 3.0\n                    else:\n                        agent['alive'] = other['alive'] = False\n                        rewards[i] = rewards[j] = -3.0\n                    break\n                \n                # Head into body\n                if (hx, hy) in set(other['segments'][1:]):\n                    agent['alive'] = False\n                    rewards[i] -= 5.0\n                    rewards[j] += 2.0\n                    break\n        \n        # Food\n        for i, agent in enumerate(self.agents):\n            if not agent['alive']:\n                continue\n            head = agent['segments'][0]\n            if head in self.pickup_set:\n                self.pickup_set.remove(head)\n                agent['segments'].append(agent['segments'][-1])\n                agent['score'] += 1\n                rewards[i] += 1.0\n                self.pickup_set.add((\n                    random.randint(0, self.WORLD_COLS - 1),\n                    random.randint(0, self.WORLD_ROWS - 1)\n                ))\n        \n        observations = [self.get_vision_fast(i) for i in range(self.num_agents)]\n        dones = [not a['alive'] for a in self.agents]\n        \n        alive = sum(1 for a in self.agents if a['alive'])\n        game_over = alive <= 1 or self.step_count >= 500  # Shorter episodes\n        \n        if game_over and alive == 1:\n            for i, a in enumerate(self.agents):\n                if a['alive']:\n                    rewards[i] += 5.0\n        \n        return observations, rewards.tolist(), dones, game_over",
   "metadata": {
    "id": "environment"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Self-Play Training\n",
    "Uses a pool of past versions to ensure diverse opponents"
   ],
   "metadata": {
    "id": "selfplay_header"
   }
  },
  {
   "cell_type": "code",
   "source": "class SelfPlayTrainer:\n    def __init__(self, pool_size=10):\n        self.current_model = LocomotNetwork().to(device)\n        self.target_model = LocomotNetwork().to(device)\n        self.target_model.load_state_dict(self.current_model.state_dict())\n        \n        self.opponent_pool = [deepcopy(self.current_model.state_dict())]\n        self.pool_size = pool_size\n        self.memory = deque(maxlen=50000)\n        \n        self.optimizer = optim.Adam(self.current_model.parameters(), lr=0.001)\n        self.gamma = 0.99\n        self.epsilon = 1.0\n        self.epsilon_min = 0.05\n        self.epsilon_decay = 0.995  # Faster decay\n        self.batch_size = 64\n        \n        self.episode_rewards = []\n        self.win_rates = []\n    \n    def select_opponents(self, num_opponents):\n        opponents = []\n        for _ in range(num_opponents):\n            if random.random() < 0.3:\n                opponents.append(deepcopy(self.current_model))\n            else:\n                idx = min(int(random.triangular(0, len(self.opponent_pool), len(self.opponent_pool))), \n                         len(self.opponent_pool) - 1)\n                opponent = LocomotNetwork().to(device)\n                opponent.load_state_dict(self.opponent_pool[idx])\n                opponent.eval()\n                opponents.append(opponent)\n        return opponents\n    \n    def play_episode(self):\n        env = LocomotEnv(num_agents=4)\n        observations = env.reset()\n        opponents = self.select_opponents(3)\n        \n        episode_transitions = []\n        total_reward = 0\n        \n        while True:\n            actions = [self.current_model.get_action(observations[0], self.epsilon)]\n            for i, opp in enumerate(opponents):\n                if env.agents[i + 1]['alive']:\n                    actions.append(opp.get_action(observations[i + 1], 0.0))\n                else:\n                    actions.append(1)\n            \n            next_obs, rewards, dones, game_over = env.step(actions)\n            \n            if env.agents[0]['alive'] or dones[0]:\n                episode_transitions.append((observations[0], actions[0], rewards[0], next_obs[0], dones[0]))\n                total_reward += rewards[0]\n            \n            observations = next_obs\n            if game_over:\n                break\n        \n        for t in episode_transitions:\n            self.memory.append(t)\n        \n        return total_reward, env.agents[0]['alive'], len(env.agents[0]['segments'])\n    \n    def train_step(self):\n        if len(self.memory) < self.batch_size:\n            return 0\n        \n        batch = random.sample(self.memory, self.batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n        \n        states = torch.FloatTensor(np.array(states)).to(device)\n        actions = torch.LongTensor(actions).to(device)\n        rewards = torch.FloatTensor(rewards).to(device)\n        next_states = torch.FloatTensor(np.array(next_states)).to(device)\n        dones = torch.FloatTensor(dones).to(device)\n        \n        current_q = self.current_model(states).gather(1, actions.unsqueeze(1))\n        \n        with torch.no_grad():\n            next_actions = self.current_model(next_states).argmax(1)\n            next_q = self.target_model(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n            target_q = rewards + self.gamma * next_q * (1 - dones)\n        \n        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.current_model.parameters(), 1.0)\n        self.optimizer.step()\n        return loss.item()\n    \n    def update_target(self):\n        tau = 0.01\n        for tp, cp in zip(self.target_model.parameters(), self.current_model.parameters()):\n            tp.data.copy_(tau * cp.data + (1 - tau) * tp.data)\n    \n    def add_to_pool(self):\n        self.opponent_pool.append(deepcopy(self.current_model.state_dict()))\n        if len(self.opponent_pool) > self.pool_size:\n            del self.opponent_pool[len(self.opponent_pool) // 2]\n    \n    def train(self, num_episodes=2000, save_every=500):\n        \"\"\"Main training loop with progress tracking\"\"\"\n        import time\n        wins = 0\n        recent_rewards = deque(maxlen=50)\n        start_time = time.time()\n        \n        for episode in range(num_episodes):\n            reward, won, final_length = self.play_episode()\n            recent_rewards.append(reward)\n            if won:\n                wins += 1\n            \n            for _ in range(2):\n                self.train_step()\n            \n            self.update_target()\n            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n            \n            if episode % 50 == 0 and episode > 0:\n                self.add_to_pool()\n            \n            # Print every 25 episodes for faster feedback\n            if episode % 25 == 0:\n                elapsed = time.time() - start_time\n                eps_per_sec = (episode + 1) / elapsed if elapsed > 0 else 0\n                win_rate = wins / 25 if episode > 0 else 0\n                avg_reward = np.mean(recent_rewards) if recent_rewards else 0\n                print(f'Ep {episode:4d} | Reward: {avg_reward:6.2f} | Win: {win_rate:5.1%} | ε: {self.epsilon:.3f} | {eps_per_sec:.1f} ep/s')\n                self.episode_rewards.append(avg_reward)\n                self.win_rates.append(win_rate)\n                wins = 0\n            \n            if episode % save_every == 0 and episode > 0:\n                save_brain_json(self.current_model, f'brain_selfplay_ep{episode}.json')\n        \n        print(f'\\nDone! Total time: {time.time() - start_time:.1f}s')\n        return self.current_model",
   "metadata": {
    "id": "selfplay"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Training"
   ],
   "metadata": {
    "id": "run_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Create trainer\ntrainer = SelfPlayTrainer(pool_size=10)\n\n# Train! (2000 episodes should take ~5-10 min)\ntrained_model = trainer.train(num_episodes=2000, save_every=500)",
   "metadata": {
    "id": "run_training"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot training progress\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(trainer.episode_rewards)\n",
    "ax1.set_title('Average Episode Reward')\n",
    "ax1.set_xlabel('Episode (×100)')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(trainer.win_rates)\n",
    "ax2.set_title('Win Rate vs Self-Play Pool')\n",
    "ax2.set_xlabel('Episode (×100)')\n",
    "ax2.set_ylabel('Win Rate')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "plot"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Save final brain for use in game\n",
    "save_brain_json(trained_model, 'brain_selfplay_final.json')\n",
    "print('\\nDone! Download brain_selfplay_final.json and replace the BRAIN data in index.html')"
   ],
   "metadata": {
    "id": "save"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How to Use the Trained Brain\n",
    "\n",
    "1. Download `brain_selfplay_final.json`\n",
    "2. Open `index.html`\n",
    "3. Find the `<script id=\"brainData\" type=\"application/json\">` section\n",
    "4. Replace its contents with the downloaded JSON\n",
    "5. Deploy and test!\n",
    "\n",
    "The self-play trained AI should be more aggressive and strategic than the original."
   ],
   "metadata": {
    "id": "howto"
   }
  }
 ]
}