{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# LOCOMOT.IO Self-Play Training v2\n\nTrain the neural network AI by playing against itself. Creates a curriculum of progressively harder opponents.\n\n**Architecture v2 (Extended Inputs):**\n- Input: 60 values\n  - 56 spatial (8 directions × 7 features: health_pickup, gun_pickup, self, wall, smaller_head, bigger_head, enemy_body)\n  - 4 state: health_ratio, arena_position, threat_density, my_length\n- Hidden: 64 → 64 (ReLU)\n- Output: 3 (left, straight, right)\n\n**New Features:**\n- **Transfer Learning**: Can load old 48-input brain and adapt weights\n- **Health-aware**: Distinguishes health pickups from gun pickups\n- **State-aware**: Knows its health, position, threats, and length\n\n**Self-Play Strategy:**\n1. Start with current trained model (or transfer from old brain)\n2. Play games against copies of itself\n3. Learn from wins and losses\n4. Keep a pool of past versions for diverse training",
   "metadata": {
    "id": "intro"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network Architecture\n",
    "Matches the JavaScript implementation exactly"
   ],
   "metadata": {
    "id": "nn_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Architecture constants\nOLD_INPUT_SIZE = 48  # 8 directions × 6 features\nNEW_INPUT_SIZE = 60  # 8 directions × 7 features + 4 state inputs\nHIDDEN_SIZE = 64\nOUTPUT_SIZE = 3\n\nclass LocomotNetwork(nn.Module):\n    def __init__(self, input_size=NEW_INPUT_SIZE):\n        super().__init__()\n        self.input_size = input_size\n        self.net = nn.Sequential(\n            nn.Linear(input_size, HIDDEN_SIZE),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n    \n    def get_action(self, state, epsilon=0.0):\n        \"\"\"Get action with epsilon-greedy exploration\"\"\"\n        if random.random() < epsilon:\n            return random.randint(0, 2)\n        with torch.no_grad():\n            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n            q_values = self.forward(state_t)\n            return q_values.argmax(dim=1).item()\n\n\ndef load_existing_weights(model, json_path):\n    \"\"\"Load weights from JavaScript brain.json format (same input size)\"\"\"\n    with open(json_path, 'r') as f:\n        weights = json.load(f)\n    \n    state_dict = {\n        'net.0.weight': torch.FloatTensor(weights['net.0.weight']),\n        'net.0.bias': torch.FloatTensor(weights['net.0.bias']),\n        'net.2.weight': torch.FloatTensor(weights['net.2.weight']),\n        'net.2.bias': torch.FloatTensor(weights['net.2.bias']),\n        'net.4.weight': torch.FloatTensor(weights['net.4.weight']),\n        'net.4.bias': torch.FloatTensor(weights['net.4.bias']),\n    }\n    model.load_state_dict(state_dict)\n    return model\n\n\ndef transfer_from_old_brain(new_model, old_json_path):\n    \"\"\"\n    Transfer learning: Load old 48-input brain weights into new 60-input model.\n    \n    Old format (48 inputs = 8 dirs × 6 features):\n      Per direction: [food, self, wall, smaller_head, bigger_head, enemy_body]\n    \n    New format (60 inputs = 8 dirs × 7 features + 4 state):\n      Per direction: [health_pickup, gun_pickup, self, wall, smaller_head, bigger_head, enemy_body]\n      State: [health_ratio, arena_position, threat_density, my_length]\n    \n    Mapping strategy:\n    - Old 'food' weights → average into both health_pickup and gun_pickup\n    - Old self/wall/enemy weights → copy directly to new positions\n    - New state inputs → initialize with small random weights\n    \"\"\"\n    with open(old_json_path, 'r') as f:\n        old_weights = json.load(f)\n    \n    old_w0 = torch.FloatTensor(old_weights['net.0.weight'])  # [64, 48]\n    old_b0 = torch.FloatTensor(old_weights['net.0.bias'])    # [64]\n    \n    # Create new first layer weights [64, 60]\n    new_w0 = torch.zeros(HIDDEN_SIZE, NEW_INPUT_SIZE)\n    \n    # Map old weights to new positions\n    # Old: 8 dirs × [food, self, wall, smaller, bigger, body] = indices 0-47\n    # New: 8 dirs × [health, gun, self, wall, smaller, bigger, body] + 4 state = indices 0-59\n    \n    for d in range(8):  # 8 directions\n        old_base = d * 6  # Old: 6 features per direction\n        new_base = d * 7  # New: 7 features per direction\n        \n        # Old food → split to health_pickup and gun_pickup (indices 0, 1)\n        food_weights = old_w0[:, old_base + 0]\n        new_w0[:, new_base + 0] = food_weights  # health_pickup\n        new_w0[:, new_base + 1] = food_weights  # gun_pickup (same initially)\n        \n        # self (1→2), wall (2→3), smaller (3→4), bigger (4→5), body (5→6)\n        new_w0[:, new_base + 2] = old_w0[:, old_base + 1]  # self\n        new_w0[:, new_base + 3] = old_w0[:, old_base + 2]  # wall\n        new_w0[:, new_base + 4] = old_w0[:, old_base + 3]  # smaller_head\n        new_w0[:, new_base + 5] = old_w0[:, old_base + 4]  # bigger_head\n        new_w0[:, new_base + 6] = old_w0[:, old_base + 5]  # enemy_body\n    \n    # Initialize new state inputs (indices 56-59) with small random weights\n    # These will be learned during training\n    nn.init.xavier_uniform_(new_w0[:, 56:60])\n    \n    # Load into new model\n    new_state_dict = new_model.state_dict()\n    new_state_dict['net.0.weight'] = new_w0\n    new_state_dict['net.0.bias'] = old_b0  # Bias stays same size\n    \n    # Copy remaining layers directly (they stay the same size)\n    new_state_dict['net.2.weight'] = torch.FloatTensor(old_weights['net.2.weight'])\n    new_state_dict['net.2.bias'] = torch.FloatTensor(old_weights['net.2.bias'])\n    new_state_dict['net.4.weight'] = torch.FloatTensor(old_weights['net.4.weight'])\n    new_state_dict['net.4.bias'] = torch.FloatTensor(old_weights['net.4.bias'])\n    \n    new_model.load_state_dict(new_state_dict)\n    print(f\"✓ Transferred weights from old 48-input brain to new 60-input model\")\n    print(f\"  - Spatial features mapped (food → health+gun)\")\n    print(f\"  - State inputs initialized with Xavier uniform\")\n    return new_model\n\n\ndef save_brain_json(model, path):\n    \"\"\"Export weights to JavaScript-compatible format\"\"\"\n    state_dict = model.state_dict()\n    brain = {\n        'input_size': model.input_size,  # Store input size for compatibility checking\n        'net.0.weight': state_dict['net.0.weight'].cpu().numpy().tolist(),\n        'net.0.bias': state_dict['net.0.bias'].cpu().numpy().tolist(),\n        'net.2.weight': state_dict['net.2.weight'].cpu().numpy().tolist(),\n        'net.2.bias': state_dict['net.2.bias'].cpu().numpy().tolist(),\n        'net.4.weight': state_dict['net.4.weight'].cpu().numpy().tolist(),\n        'net.4.bias': state_dict['net.4.bias'].cpu().numpy().tolist(),\n    }\n    with open(path, 'w') as f:\n        json.dump(brain, f)\n    print(f'Saved brain to {path} (input_size={model.input_size})')",
   "metadata": {
    "id": "network"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Game Environment\n",
    "Simplified version of LOCOMOT.IO for fast training"
   ],
   "metadata": {
    "id": "env_header"
   }
  },
  {
   "cell_type": "code",
   "source": "class LocomotEnv:\n    \"\"\"FAST simplified LOCOMOT.IO environment for self-play with extended inputs\"\"\"\n    \n    WORLD_COLS = 50  # Smaller world = faster\n    WORLD_ROWS = 40\n    DIRECTIONS = [(0, -1), (1, 0), (0, 1), (-1, 0)]  # UP, RIGHT, DOWN, LEFT\n    \n    # Pre-compute direction offsets for 8 rays\n    RAY_OFFSETS = [\n        (0, -1), (1, -1), (1, 0), (1, 1),\n        (0, 1), (-1, 1), (-1, 0), (-1, -1)\n    ]\n    \n    def __init__(self, num_agents=4):\n        self.num_agents = num_agents\n        self.reset()\n    \n    def reset(self):\n        self.agents = []\n        self.health_pickups = set()  # Health pickups (green)\n        self.gun_pickups = set()     # Gun pickups (various colors)\n        self.segment_map = {}  # (x,y) -> (agent_idx, seg_idx) for fast collision\n        self.step_count = 0\n        \n        # Spawn agents with health tracking\n        for i in range(self.num_agents):\n            x = random.randint(8, self.WORLD_COLS - 8)\n            y = random.randint(8, self.WORLD_ROWS - 8)\n            direction = random.randint(0, 3)\n            dx, dy = self.DIRECTIONS[direction]\n            \n            # Each segment has hp/maxHp (head is invincible)\n            segments = []\n            for j in range(4):\n                seg = {\n                    'x': x - j*dx,\n                    'y': y - j*dy,\n                    'hp': 100 if j > 0 else float('inf'),  # Head invincible\n                    'maxHp': 100 if j > 0 else float('inf')\n                }\n                segments.append(seg)\n            \n            self.agents.append({\n                'segments': segments,\n                'direction': direction,\n                'alive': True,\n                'score': 0\n            })\n            \n            # Add to segment map\n            for seg_idx, seg in enumerate(segments):\n                self.segment_map[(seg['x'], seg['y'])] = (i, seg_idx)\n        \n        # Spawn pickups (70% gun, 30% health like the real game)\n        for _ in range(15):\n            self.gun_pickups.add((\n                random.randint(0, self.WORLD_COLS - 1),\n                random.randint(0, self.WORLD_ROWS - 1)\n            ))\n        for _ in range(5):\n            self.health_pickups.add((\n                random.randint(0, self.WORLD_COLS - 1),\n                random.randint(0, self.WORLD_ROWS - 1)\n            ))\n        \n        return [self.get_vision_v2(i) for i in range(self.num_agents)]\n    \n    def get_health_ratio(self, agent):\n        \"\"\"Calculate health ratio (0-1) from body segments\"\"\"\n        total_hp = 0\n        total_max = 0\n        for seg in agent['segments'][1:]:  # Skip head (infinite hp)\n            if seg['hp'] != float('inf'):\n                total_hp += seg['hp']\n                total_max += seg['maxHp']\n        return total_hp / total_max if total_max > 0 else 1.0\n    \n    def get_arena_position(self, head_x, head_y):\n        \"\"\"Calculate arena position safety (0-1, higher = more centered)\"\"\"\n        dist_to_edge = min(head_x, head_y, \n                          self.WORLD_COLS - 1 - head_x, \n                          self.WORLD_ROWS - 1 - head_y)\n        max_dist = min(self.WORLD_COLS, self.WORLD_ROWS) / 2\n        return min(dist_to_edge / max_dist, 1.0)\n    \n    def get_threat_density(self, agent_idx, head_x, head_y):\n        \"\"\"Calculate threat density from nearby enemies (0-1)\"\"\"\n        threat = 0.0\n        my_length = len(self.agents[agent_idx]['segments'])\n        \n        for i, other in enumerate(self.agents):\n            if i == agent_idx or not other['alive']:\n                continue\n            \n            other_head = other['segments'][0]\n            dist = abs(other_head['x'] - head_x) + abs(other_head['y'] - head_y)\n            \n            if dist < 15:\n                # Bigger enemies are more threatening\n                size_factor = 2.0 if len(other['segments']) > my_length else 0.5\n                threat += size_factor / max(dist, 1)\n        \n        # Normalize to 0-1 (cap at reasonable max)\n        return min(threat / 3.0, 1.0)\n    \n    def get_vision_v2(self, agent_idx):\n        \"\"\"\n        Extended vision with 60 inputs:\n        - 56 spatial: 8 directions × 7 features\n          [health_pickup, gun_pickup, self, wall, smaller_head, bigger_head, enemy_body]\n        - 4 state: [health_ratio, arena_position, threat_density, my_length_normalized]\n        \"\"\"\n        agent = self.agents[agent_idx]\n        if not agent['alive']:\n            return np.zeros(NEW_INPUT_SIZE, dtype=np.float32)\n        \n        head = agent['segments'][0]\n        head_x, head_y = head['x'], head['y']\n        current_dir = agent['direction']\n        my_length = len(agent['segments'])\n        my_segments = set((s['x'], s['y']) for s in agent['segments'][1:])\n        \n        vision = np.zeros(NEW_INPUT_SIZE, dtype=np.float32)\n        \n        # Rotate ray offsets based on direction\n        rot = current_dir\n        \n        for ray_idx in range(8):\n            rotated_idx = (ray_idx + rot * 2) % 8\n            dx, dy = self.RAY_OFFSETS[rotated_idx]\n            \n            health_dist = gun_dist = self_danger = wall_dist = 0.0\n            enemy_smaller = enemy_bigger = enemy_body = 0.0\n            \n            for dist in range(1, 16):\n                cx, cy = head_x + dx * dist, head_y + dy * dist\n                \n                # Wall\n                if cx < 0 or cx >= self.WORLD_COLS or cy < 0 or cy >= self.WORLD_ROWS:\n                    if wall_dist == 0:\n                        wall_dist = 1.0 / dist\n                    break\n                \n                # Self body\n                if self_danger == 0 and (cx, cy) in my_segments:\n                    self_danger = 1.0 / dist\n                \n                # Health pickup\n                if health_dist == 0 and (cx, cy) in self.health_pickups:\n                    health_dist = 1.0 / dist\n                \n                # Gun pickup\n                if gun_dist == 0 and (cx, cy) in self.gun_pickups:\n                    gun_dist = 1.0 / dist\n                \n                # Other agents\n                pos = (cx, cy)\n                if pos in self.segment_map:\n                    other_idx, seg_idx = self.segment_map[pos]\n                    if other_idx != agent_idx and self.agents[other_idx]['alive']:\n                        other_len = len(self.agents[other_idx]['segments'])\n                        if seg_idx == 0:  # Head\n                            if other_len < my_length and enemy_smaller == 0:\n                                enemy_smaller = 1.0 / dist\n                            elif enemy_bigger == 0:\n                                enemy_bigger = 1.0 / dist\n                        elif enemy_body == 0:\n                            enemy_body = 1.0 / dist\n            \n            # Store 7 features per direction\n            base = ray_idx * 7\n            vision[base:base+7] = [health_dist, gun_dist, self_danger, wall_dist, \n                                   enemy_smaller, enemy_bigger, enemy_body]\n        \n        # Add 4 state inputs (indices 56-59)\n        vision[56] = self.get_health_ratio(agent)\n        vision[57] = self.get_arena_position(head_x, head_y)\n        vision[58] = self.get_threat_density(agent_idx, head_x, head_y)\n        vision[59] = min(my_length / 20.0, 1.0)  # Normalized length (cap at 20)\n        \n        return vision\n    \n    def step(self, actions):\n        \"\"\"Execute actions. Returns (observations, rewards, dones, game_over)\"\"\"\n        self.step_count += 1\n        rewards = np.zeros(self.num_agents, dtype=np.float32)\n        \n        # Apply turns\n        for i, agent in enumerate(self.agents):\n            if not agent['alive']:\n                continue\n            action = actions[i]\n            if action == 0:\n                agent['direction'] = (agent['direction'] - 1) % 4\n            elif action == 2:\n                agent['direction'] = (agent['direction'] + 1) % 4\n        \n        # Move agents and update segment map\n        for i, agent in enumerate(self.agents):\n            if not agent['alive']:\n                continue\n            \n            # Remove old tail from map\n            old_tail = agent['segments'][-1]\n            tail_pos = (old_tail['x'], old_tail['y'])\n            if tail_pos in self.segment_map and self.segment_map[tail_pos][0] == i:\n                del self.segment_map[tail_pos]\n            \n            dx, dy = self.DIRECTIONS[agent['direction']]\n            head = agent['segments'][0]\n            new_head = {\n                'x': head['x'] + dx,\n                'y': head['y'] + dy,\n                'hp': float('inf'),\n                'maxHp': float('inf')\n            }\n            \n            agent['segments'].insert(0, new_head)\n            agent['segments'].pop()\n            \n            # Update segment map\n            for seg_idx, seg in enumerate(agent['segments']):\n                self.segment_map[(seg['x'], seg['y'])] = (i, seg_idx)\n            \n            rewards[i] += 0.01\n        \n        # Check collisions\n        for i, agent in enumerate(self.agents):\n            if not agent['alive']:\n                continue\n            \n            head = agent['segments'][0]\n            hx, hy = head['x'], head['y']\n            \n            # Wall\n            if hx < 0 or hx >= self.WORLD_COLS or hy < 0 or hy >= self.WORLD_ROWS:\n                agent['alive'] = False\n                rewards[i] -= 5.0\n                continue\n            \n            # Self collision\n            my_body = set((s['x'], s['y']) for s in agent['segments'][1:])\n            if (hx, hy) in my_body:\n                agent['alive'] = False\n                rewards[i] -= 5.0\n                continue\n            \n            # Enemy collision\n            for j, other in enumerate(self.agents):\n                if i == j or not other['alive']:\n                    continue\n                \n                oh = other['segments'][0]\n                if hx == oh['x'] and hy == oh['y']:\n                    # Head-to-head\n                    if len(agent['segments']) > len(other['segments']):\n                        other['alive'] = False\n                        rewards[j] -= 5.0\n                        rewards[i] += 3.0\n                    elif len(agent['segments']) < len(other['segments']):\n                        agent['alive'] = False\n                        rewards[i] -= 5.0\n                        rewards[j] += 3.0\n                    else:\n                        agent['alive'] = other['alive'] = False\n                        rewards[i] = rewards[j] = -3.0\n                    break\n                \n                # Head into body\n                other_body = set((s['x'], s['y']) for s in other['segments'][1:])\n                if (hx, hy) in other_body:\n                    agent['alive'] = False\n                    rewards[i] -= 5.0\n                    rewards[j] += 2.0\n                    break\n        \n        # Pickup collection\n        for i, agent in enumerate(self.agents):\n            if not agent['alive']:\n                continue\n            \n            head = agent['segments'][0]\n            head_pos = (head['x'], head['y'])\n            \n            # Health pickup - heals segments\n            if head_pos in self.health_pickups:\n                self.health_pickups.remove(head_pos)\n                # Heal all body segments\n                for seg in agent['segments'][1:]:\n                    if seg['hp'] != float('inf'):\n                        seg['hp'] = min(seg['hp'] + 30, seg['maxHp'])\n                rewards[i] += 0.5\n                # Respawn\n                self.health_pickups.add((\n                    random.randint(0, self.WORLD_COLS - 1),\n                    random.randint(0, self.WORLD_ROWS - 1)\n                ))\n            \n            # Gun pickup - grows snake\n            if head_pos in self.gun_pickups:\n                self.gun_pickups.remove(head_pos)\n                tail = agent['segments'][-1]\n                agent['segments'].append({\n                    'x': tail['x'],\n                    'y': tail['y'],\n                    'hp': 100,\n                    'maxHp': 100\n                })\n                agent['score'] += 1\n                rewards[i] += 1.0\n                # Respawn\n                self.gun_pickups.add((\n                    random.randint(0, self.WORLD_COLS - 1),\n                    random.randint(0, self.WORLD_ROWS - 1)\n                ))\n        \n        # Random damage to simulate combat\n        if self.step_count % 20 == 0:\n            for agent in self.agents:\n                if agent['alive'] and len(agent['segments']) > 1:\n                    # Random segment takes minor damage\n                    idx = random.randint(1, len(agent['segments']) - 1)\n                    if agent['segments'][idx]['hp'] != float('inf'):\n                        agent['segments'][idx]['hp'] -= random.randint(5, 15)\n                        if agent['segments'][idx]['hp'] <= 0:\n                            # Segment destroyed - snake shrinks\n                            agent['segments'].pop(idx)\n        \n        observations = [self.get_vision_v2(i) for i in range(self.num_agents)]\n        dones = [not a['alive'] for a in self.agents]\n        \n        alive = sum(1 for a in self.agents if a['alive'])\n        game_over = alive <= 1 or self.step_count >= 500\n        \n        if game_over and alive == 1:\n            for i, a in enumerate(self.agents):\n                if a['alive']:\n                    rewards[i] += 5.0\n        \n        return observations, rewards.tolist(), dones, game_over",
   "metadata": {
    "id": "environment"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Self-Play Training\n",
    "Uses a pool of past versions to ensure diverse opponents"
   ],
   "metadata": {
    "id": "selfplay_header"
   }
  },
  {
   "cell_type": "code",
   "source": "class SelfPlayTrainer:\n    def __init__(self, pool_size=10, old_brain_path=None):\n        \"\"\"\n        Initialize trainer.\n        \n        Args:\n            pool_size: Number of past model versions to keep for diverse training\n            old_brain_path: Path to old 48-input brain.json for transfer learning\n        \"\"\"\n        self.current_model = LocomotNetwork(input_size=NEW_INPUT_SIZE).to(device)\n        \n        # Transfer learning from old brain\n        if old_brain_path:\n            transfer_from_old_brain(self.current_model, old_brain_path)\n            print(f\"Starting with transferred weights from {old_brain_path}\")\n        else:\n            print(\"Starting with random initialization\")\n        \n        self.target_model = LocomotNetwork(input_size=NEW_INPUT_SIZE).to(device)\n        self.target_model.load_state_dict(self.current_model.state_dict())\n        \n        self.opponent_pool = [deepcopy(self.current_model.state_dict())]\n        self.pool_size = pool_size\n        self.memory = deque(maxlen=50000)\n        \n        self.optimizer = optim.Adam(self.current_model.parameters(), lr=0.001)\n        self.gamma = 0.99\n        self.epsilon = 0.5 if old_brain_path else 1.0  # Less exploration if transferring\n        self.epsilon_min = 0.05\n        self.epsilon_decay = 0.995\n        self.batch_size = 64\n        \n        self.episode_rewards = []\n        self.win_rates = []\n    \n    def select_opponents(self, num_opponents):\n        opponents = []\n        for _ in range(num_opponents):\n            if random.random() < 0.3:\n                opponents.append(deepcopy(self.current_model))\n            else:\n                idx = min(int(random.triangular(0, len(self.opponent_pool), len(self.opponent_pool))), \n                         len(self.opponent_pool) - 1)\n                opponent = LocomotNetwork(input_size=NEW_INPUT_SIZE).to(device)\n                opponent.load_state_dict(self.opponent_pool[idx])\n                opponent.eval()\n                opponents.append(opponent)\n        return opponents\n    \n    def play_episode(self):\n        env = LocomotEnv(num_agents=4)\n        observations = env.reset()\n        opponents = self.select_opponents(3)\n        \n        episode_transitions = []\n        total_reward = 0\n        \n        while True:\n            actions = [self.current_model.get_action(observations[0], self.epsilon)]\n            for i, opp in enumerate(opponents):\n                if env.agents[i + 1]['alive']:\n                    actions.append(opp.get_action(observations[i + 1], 0.0))\n                else:\n                    actions.append(1)\n            \n            next_obs, rewards, dones, game_over = env.step(actions)\n            \n            if env.agents[0]['alive'] or dones[0]:\n                episode_transitions.append((observations[0], actions[0], rewards[0], next_obs[0], dones[0]))\n                total_reward += rewards[0]\n            \n            observations = next_obs\n            if game_over:\n                break\n        \n        for t in episode_transitions:\n            self.memory.append(t)\n        \n        # Return health ratio at end for monitoring\n        final_health = env.get_health_ratio(env.agents[0]) if env.agents[0]['alive'] else 0\n        return total_reward, env.agents[0]['alive'], len(env.agents[0]['segments']), final_health\n    \n    def train_step(self):\n        if len(self.memory) < self.batch_size:\n            return 0\n        \n        batch = random.sample(self.memory, self.batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n        \n        states = torch.FloatTensor(np.array(states)).to(device)\n        actions = torch.LongTensor(actions).to(device)\n        rewards = torch.FloatTensor(rewards).to(device)\n        next_states = torch.FloatTensor(np.array(next_states)).to(device)\n        dones = torch.FloatTensor(dones).to(device)\n        \n        current_q = self.current_model(states).gather(1, actions.unsqueeze(1))\n        \n        with torch.no_grad():\n            next_actions = self.current_model(next_states).argmax(1)\n            next_q = self.target_model(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n            target_q = rewards + self.gamma * next_q * (1 - dones)\n        \n        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.current_model.parameters(), 1.0)\n        self.optimizer.step()\n        return loss.item()\n    \n    def update_target(self):\n        tau = 0.01\n        for tp, cp in zip(self.target_model.parameters(), self.current_model.parameters()):\n            tp.data.copy_(tau * cp.data + (1 - tau) * tp.data)\n    \n    def add_to_pool(self):\n        self.opponent_pool.append(deepcopy(self.current_model.state_dict()))\n        if len(self.opponent_pool) > self.pool_size:\n            del self.opponent_pool[len(self.opponent_pool) // 2]\n    \n    def train(self, num_episodes=2000, save_every=500):\n        \"\"\"Main training loop with progress tracking\"\"\"\n        import time\n        wins = 0\n        recent_rewards = deque(maxlen=50)\n        recent_health = deque(maxlen=50)\n        start_time = time.time()\n        \n        for episode in range(num_episodes):\n            reward, won, final_length, final_health = self.play_episode()\n            recent_rewards.append(reward)\n            recent_health.append(final_health)\n            if won:\n                wins += 1\n            \n            for _ in range(2):\n                self.train_step()\n            \n            self.update_target()\n            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n            \n            if episode % 50 == 0 and episode > 0:\n                self.add_to_pool()\n            \n            if episode % 25 == 0:\n                elapsed = time.time() - start_time\n                eps_per_sec = (episode + 1) / elapsed if elapsed > 0 else 0\n                win_rate = wins / 25 if episode > 0 else 0\n                avg_reward = np.mean(recent_rewards) if recent_rewards else 0\n                avg_health = np.mean(recent_health) if recent_health else 0\n                print(f'Ep {episode:4d} | R: {avg_reward:6.2f} | Win: {win_rate:5.1%} | HP: {avg_health:.2f} | ε: {self.epsilon:.3f} | {eps_per_sec:.1f} ep/s')\n                self.episode_rewards.append(avg_reward)\n                self.win_rates.append(win_rate)\n                wins = 0\n            \n            if episode % save_every == 0 and episode > 0:\n                save_brain_json(self.current_model, f'brain_v2_ep{episode}.json')\n        \n        print(f'\\nDone! Total time: {time.time() - start_time:.1f}s')\n        return self.current_model",
   "metadata": {
    "id": "selfplay"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Training\n\n**Option 1: Train from scratch**\n```python\ntrainer = SelfPlayTrainer(pool_size=10)\n```\n\n**Option 2: Transfer from old brain (RECOMMENDED)**\n```python\ntrainer = SelfPlayTrainer(pool_size=10, old_brain_path='path/to/old_brain.json')\n```\n\nThis loads the old 48-input weights and maps them to the new 60-input architecture.",
   "metadata": {
    "id": "run_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# OPTION 1: Train from scratch (new random weights)\n# trainer = SelfPlayTrainer(pool_size=10)\n\n# OPTION 2: Transfer learning from existing brain (RECOMMENDED)\n# Extract the current brain from index.html's brainData script tag, save as old_brain.json\ntrainer = SelfPlayTrainer(pool_size=10, old_brain_path='old_brain.json')\n\n# Train! (2000 episodes should take ~5-10 min)\ntrained_model = trainer.train(num_episodes=2000, save_every=500)",
   "metadata": {
    "id": "run_training"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot training progress\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(trainer.episode_rewards)\n",
    "ax1.set_title('Average Episode Reward')\n",
    "ax1.set_xlabel('Episode (×100)')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(trainer.win_rates)\n",
    "ax2.set_title('Win Rate vs Self-Play Pool')\n",
    "ax2.set_xlabel('Episode (×100)')\n",
    "ax2.set_ylabel('Win Rate')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "plot"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save final brain for use in game\nsave_brain_json(trained_model, 'brain_v2_final.json')\nprint('\\nDone! The new brain has 60 inputs instead of 48.')\nprint('You will need to update getVision() in index.html to match the new format.')",
   "metadata": {
    "id": "save"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## How to Use the Trained Brain v2\n\n### 1. Extract Current Brain (for transfer learning)\n```javascript\n// In browser console on locomot.io:\nconsole.log(JSON.stringify(BRAIN));\n// Copy output to old_brain.json\n```\n\n### 2. Train with Transfer Learning\n```python\ntrainer = SelfPlayTrainer(pool_size=10, old_brain_path='old_brain.json')\ntrained = trainer.train(num_episodes=2000)\nsave_brain_json(trained, 'brain_v2_final.json')\n```\n\n### 3. Update index.html\nThe new brain expects 60 inputs instead of 48. You need to update `getVision()`:\n\n**Old format (48 inputs = 8 dirs × 6 features):**\n```\nPer direction: [food, self, wall, smaller_head, bigger_head, enemy_body]\n```\n\n**New format (60 inputs = 8 dirs × 7 features + 4 state):**\n```\nPer direction: [health_pickup, gun_pickup, self, wall, smaller_head, bigger_head, enemy_body]\nState inputs: [health_ratio, arena_position, threat_density, my_length_normalized]\n```\n\n### 4. JavaScript Changes Needed\n```javascript\n// In getVision(), change from:\ninput[i * 6 + 0] = foodDist;\n// To:\ninput[i * 7 + 0] = healthPickupDist;\ninput[i * 7 + 1] = gunPickupDist;\n// ... etc\n\n// Add state inputs at the end:\ninput[56] = healthRatio;      // sum(seg.hp) / sum(seg.maxHp)\ninput[57] = arenaPosition;    // min dist to edge / 20, capped at 1\ninput[58] = threatDensity;    // nearby enemy threat, 0-1\ninput[59] = myLength / 20;    // normalized length\n```\n\n### 5. Replace Brain Data\nReplace the `<script id=\"brainData\">` contents with `brain_v2_final.json`",
   "metadata": {
    "id": "howto"
   }
  }
 ]
}