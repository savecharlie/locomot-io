{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# LOCOMOT.IO Neural Network Training v2\n\n## Quick Start (Works in Colab!)\n1. Runtime → Run all\n2. Upload `old_brain.json` when prompted\n3. Wait ~5-10 min for training\n4. Download `brain_v2_final.json` when done\n\n**Two training modes:**\n- **Self-play**: AI learns by playing against itself\n- **Imitation**: AI learns from recorded top player data",
   "metadata": {
    "id": "intro"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": "# Setup - works in Colab or locally\nimport subprocess\nimport sys\n\n# Install dependencies if needed (Colab)\ntry:\n    import torch\n    from tqdm.auto import tqdm\nexcept ImportError:\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'torch', 'numpy', 'matplotlib', 'tqdm', '-q'])\n    from tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nimport json\nfrom collections import deque\nfrom copy import deepcopy\nimport matplotlib.pyplot as plt\nimport os\nfrom tqdm.auto import tqdm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using: {device}')\n\n# Handle file uploads for Colab\nIN_COLAB = 'google.colab' in sys.modules\nif IN_COLAB:\n    from google.colab import files\n    print('Running in Colab - will prompt for file uploads')\nelse:\n    print('Running locally')"
  },
  {
   "cell_type": "markdown",
   "source": "## Setup (just run these cells)",
   "metadata": {
    "id": "nn_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Architecture constants\nOLD_INPUT_SIZE = 85  # Previous: 8 directions × 10 features + 5 state\nNEW_INPUT_SIZE = 96  # New: 8 directions × 11 features + 8 state (gun type + MVP)\nHIDDEN1_SIZE = 128   # Larger hidden layer for more features\nHIDDEN2_SIZE = 64\nOUTPUT_SIZE = 3\n\nclass LocomotNetwork(nn.Module):\n    def __init__(self, input_size=NEW_INPUT_SIZE):\n        super().__init__()\n        self.input_size = input_size\n        self.net = nn.Sequential(\n            nn.Linear(input_size, HIDDEN1_SIZE),\n            nn.ReLU(),\n            nn.Linear(HIDDEN1_SIZE, HIDDEN2_SIZE),\n            nn.ReLU(),\n            nn.Linear(HIDDEN2_SIZE, OUTPUT_SIZE)\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n    \n    def get_action(self, state, epsilon=0.0):\n        \"\"\"Get action with epsilon-greedy exploration\"\"\"\n        if random.random() < epsilon:\n            return random.randint(0, 2)\n        with torch.no_grad():\n            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n            q_values = self.forward(state_t)\n            return q_values.argmax(dim=1).item()\n\n\ndef load_existing_weights(model, json_path):\n    \"\"\"Load weights from JavaScript brain.json format (same input size)\"\"\"\n    with open(json_path, 'r') as f:\n        weights = json.load(f)\n    \n    state_dict = {\n        'net.0.weight': torch.FloatTensor(weights['net.0.weight']),\n        'net.0.bias': torch.FloatTensor(weights['net.0.bias']),\n        'net.2.weight': torch.FloatTensor(weights['net.2.weight']),\n        'net.2.bias': torch.FloatTensor(weights['net.2.bias']),\n        'net.4.weight': torch.FloatTensor(weights['net.4.weight']),\n        'net.4.bias': torch.FloatTensor(weights['net.4.bias']),\n    }\n    model.load_state_dict(state_dict)\n    return model\n\n\ndef transfer_from_old_brain(new_model, old_json_path):\n    \"\"\"\n    Transfer learning: Load old 85-input brain weights into new 96-input model.\n    \n    Old format (85 inputs = 8 dirs × 10 features + 5 state):\n      Per direction: [health_pickup, machinegun, shotgun, cannon, pulse, self, wall, smaller_head, bigger_head, enemy_body]\n      State: [health_ratio, arena_position, threat_density, my_length, current_gun_type]\n    \n    New format (96 inputs = 8 dirs × 11 features + 8 state):\n      Per direction: [health_pickup, machinegun, shotgun, cannon, pulse, self, wall, smaller_head, bigger_head, enemy_body, enemy_gun_type]\n      State: [health_ratio, arena_position, threat_density, my_length, current_gun_type, is_mvp, mvp_time, mvp_distance]\n    \n    Mapping strategy:\n    - Old 10 spatial features → new positions (indices 0-9)\n    - New enemy_gun_type (index 10) → initialize with small random weights\n    - Old 5 state inputs → copy to new state positions (88-92)\n    - New MVP inputs (93-95) → initialize with small random weights\n    \"\"\"\n    with open(old_json_path, 'r') as f:\n        old_weights = json.load(f)\n    \n    old_input_size = len(old_weights['net.0.weight'][0])\n    old_hidden_size = len(old_weights['net.0.bias'])\n    \n    print(f\"  Old brain: {old_input_size} inputs, {old_hidden_size} hidden\")\n    \n    old_w0 = torch.FloatTensor(old_weights['net.0.weight'])\n    old_b0 = torch.FloatTensor(old_weights['net.0.bias'])\n    \n    new_w0 = torch.zeros(HIDDEN1_SIZE, NEW_INPUT_SIZE)\n    new_b0 = torch.zeros(HIDDEN1_SIZE)\n    \n    new_b0[:min(old_hidden_size, HIDDEN1_SIZE)] = old_b0[:min(old_hidden_size, HIDDEN1_SIZE)]\n    \n    if old_input_size == 85:\n        # Transfer from 85-input brain to 96-input\n        for d in range(8):\n            old_base = d * 10  # Old: 10 features per direction\n            new_base = d * 11  # New: 11 features per direction\n            \n            for h in range(min(old_hidden_size, HIDDEN1_SIZE)):\n                # Copy first 10 features directly\n                for f in range(10):\n                    new_w0[h, new_base + f] = old_w0[h, old_base + f]\n                # Index 10 (enemy_gun_type) stays zero, will be Xavier init\n        \n        # Copy 5 state inputs (80-84 → 88-92)\n        for h in range(min(old_hidden_size, HIDDEN1_SIZE)):\n            for s in range(5):\n                new_w0[h, 88 + s] = old_w0[h, 80 + s]\n    elif old_input_size == 60:\n        # Transfer from old 60-input brain\n        for d in range(8):\n            old_base = d * 7\n            new_base = d * 11\n            \n            for h in range(min(old_hidden_size, HIDDEN1_SIZE)):\n                new_w0[h, new_base + 0] = old_w0[h, old_base + 0]  # health\n                gun_weight = old_w0[h, old_base + 1]\n                for g in range(4):\n                    new_w0[h, new_base + 1 + g] = gun_weight  # gun types\n                new_w0[h, new_base + 5] = old_w0[h, old_base + 2]  # self\n                new_w0[h, new_base + 6] = old_w0[h, old_base + 3]  # wall\n                new_w0[h, new_base + 7] = old_w0[h, old_base + 4]  # smaller\n                new_w0[h, new_base + 8] = old_w0[h, old_base + 5]  # bigger\n                new_w0[h, new_base + 9] = old_w0[h, old_base + 6]  # body\n        \n        for h in range(min(old_hidden_size, HIDDEN1_SIZE)):\n            for s in range(4):\n                new_w0[h, 88 + s] = old_w0[h, 56 + s]\n    else:\n        print(f\"  Unknown old format ({old_input_size} inputs), using random init\")\n        nn.init.xavier_uniform_(new_w0)\n    \n    # Initialize new inputs with Xavier\n    for d in range(8):\n        nn.init.xavier_uniform_(new_w0[:, d*11 + 10:d*11 + 11])  # enemy_gun_type\n    nn.init.xavier_uniform_(new_w0[:, 93:96])  # MVP inputs\n    \n    new_state_dict = new_model.state_dict()\n    new_state_dict['net.0.weight'] = new_w0\n    new_state_dict['net.0.bias'] = new_b0\n    \n    # Hidden and output layers\n    old_w2 = torch.FloatTensor(old_weights['net.2.weight'])\n    old_b2 = torch.FloatTensor(old_weights['net.2.bias'])\n    new_w2 = torch.zeros(HIDDEN2_SIZE, HIDDEN1_SIZE)\n    new_b2 = torch.zeros(HIDDEN2_SIZE)\n    \n    h2_copy = min(old_w2.shape[0], HIDDEN2_SIZE)\n    h1_copy = min(old_w2.shape[1], HIDDEN1_SIZE)\n    new_w2[:h2_copy, :h1_copy] = old_w2[:h2_copy, :h1_copy]\n    new_b2[:h2_copy] = old_b2[:h2_copy]\n    \n    if HIDDEN1_SIZE > old_w2.shape[1]:\n        nn.init.xavier_uniform_(new_w2[:, old_w2.shape[1]:])\n    \n    new_state_dict['net.2.weight'] = new_w2\n    new_state_dict['net.2.bias'] = new_b2\n    \n    old_w4 = torch.FloatTensor(old_weights['net.4.weight'])\n    old_b4 = torch.FloatTensor(old_weights['net.4.bias'])\n    new_w4 = torch.zeros(OUTPUT_SIZE, HIDDEN2_SIZE)\n    new_b4 = old_b4.clone()\n    \n    h2_copy = min(old_w4.shape[1], HIDDEN2_SIZE)\n    new_w4[:, :h2_copy] = old_w4[:, :h2_copy]\n    \n    new_state_dict['net.4.weight'] = new_w4\n    new_state_dict['net.4.bias'] = new_b4\n    \n    new_model.load_state_dict(new_state_dict)\n    print(f\"Transferred weights from old {old_input_size}-input brain to new {NEW_INPUT_SIZE}-input model\")\n    print(f\"  - Added enemy_gun_type per direction (index 10)\")\n    print(f\"  - Added MVP inputs (is_mvp, mvp_time, mvp_distance)\")\n    return new_model\n\n\ndef save_brain_json(model, path):\n    \"\"\"Export weights to JavaScript-compatible format\"\"\"\n    state_dict = model.state_dict()\n    brain = {\n        'input_size': model.input_size,\n        'net.0.weight': state_dict['net.0.weight'].cpu().numpy().tolist(),\n        'net.0.bias': state_dict['net.0.bias'].cpu().numpy().tolist(),\n        'net.2.weight': state_dict['net.2.weight'].cpu().numpy().tolist(),\n        'net.2.bias': state_dict['net.2.bias'].cpu().numpy().tolist(),\n        'net.4.weight': state_dict['net.4.weight'].cpu().numpy().tolist(),\n        'net.4.bias': state_dict['net.4.bias'].cpu().numpy().tolist(),\n    }\n    with open(path, 'w') as f:\n        json.dump(brain, f)\n    print(f'Saved brain to {path} (input_size={model.input_size})')",
   "metadata": {
    "id": "network"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Arena Rules Extraction\nAutomatically reads game rules from index.html to keep training environment in sync.",
   "metadata": {
    "id": "env_header"
   }
  },
  {
   "cell_type": "code",
   "source": "class LocomotEnv:\n    \"\"\"FAST simplified LOCOMOT.IO environment for self-play with 96 inputs (gun types + MVP)\"\"\"\n    \n    WORLD_COLS = 50  # Smaller world = faster\n    WORLD_ROWS = 40\n    DIRECTIONS = [(0, -1), (1, 0), (0, 1), (-1, 0)]  # UP, RIGHT, DOWN, LEFT\n    \n    # Pre-compute direction offsets for 8 rays\n    RAY_OFFSETS = [\n        (0, -1), (1, -1), (1, 0), (1, 1),\n        (0, 1), (-1, 1), (-1, 0), (-1, -1)\n    ]\n    \n    # Gun types\n    GUN_TYPES = ['MACHINEGUN', 'SHOTGUN', 'CANNON', 'PULSE']\n    GUN_TYPE_VALUE = {'MACHINEGUN': 0.25, 'SHOTGUN': 0.5, 'CANNON': 0.75, 'PULSE': 1.0}\n    \n    def __init__(self, num_agents=4):\n        self.num_agents = num_agents\n        self.current_mvp_idx = None  # Track which agent is MVP\n        self.reset()\n    \n    def reset(self):\n        self.agents = []\n        self.health_pickups = set()\n        self.gun_pickups = {}\n        self.segment_map = {}\n        self.step_count = 0\n        self.current_mvp_idx = None\n        \n        for i in range(self.num_agents):\n            x = random.randint(8, self.WORLD_COLS - 8)\n            y = random.randint(8, self.WORLD_ROWS - 8)\n            direction = random.randint(0, 3)\n            dx, dy = self.DIRECTIONS[direction]\n            \n            # \"All Same Gun\" - start with all MACHINEGUN\n            segments = []\n            for j in range(4):\n                seg = {\n                    'x': x - j*dx,\n                    'y': y - j*dy,\n                    'hp': 100 if j > 0 else float('inf'),\n                    'maxHp': 100 if j > 0 else float('inf'),\n                    'gun_type': 0 if j > 0 else -1  # 0 = MACHINEGUN\n                }\n                segments.append(seg)\n            \n            self.agents.append({\n                'segments': segments,\n                'direction': direction,\n                'alive': True,\n                'score': 0,\n                'prev_tail_pos': None,\n                'current_gun': 0,  # Track current gun type (All Same Gun)\n                'mvp_time': 0,     # Accumulated MVP time\n                'is_mvp': False\n            })\n            \n            for seg_idx, seg in enumerate(segments):\n                self.segment_map[(seg['x'], seg['y'])] = (i, seg_idx)\n        \n        for _ in range(15):\n            pos = (random.randint(0, self.WORLD_COLS - 1), random.randint(0, self.WORLD_ROWS - 1))\n            self.gun_pickups[pos] = random.randint(0, 3)\n        \n        for _ in range(5):\n            self.health_pickups.add((\n                random.randint(0, self.WORLD_COLS - 1),\n                random.randint(0, self.WORLD_ROWS - 1)\n            ))\n        \n        return [self.get_vision_v4(i) for i in range(self.num_agents)]\n    \n    def update_mvp(self):\n        \"\"\"Update which agent is MVP based on length and recent activity\"\"\"\n        best_score = -1\n        best_idx = None\n        \n        for i, agent in enumerate(self.agents):\n            if not agent['alive']:\n                agent['is_mvp'] = False\n                continue\n            score = len(agent['segments']) * 2 + agent['score']\n            if score > best_score:\n                best_score = score\n                best_idx = i\n        \n        # Update MVP status\n        for i, agent in enumerate(self.agents):\n            was_mvp = agent['is_mvp']\n            agent['is_mvp'] = (i == best_idx)\n            if agent['is_mvp']:\n                agent['mvp_time'] += 1  # Accumulate MVP time\n        \n        self.current_mvp_idx = best_idx\n    \n    def get_health_ratio(self, agent):\n        total_hp = 0\n        total_max = 0\n        for seg in agent['segments'][1:]:\n            if seg['hp'] != float('inf'):\n                total_hp += seg['hp']\n                total_max += seg['maxHp']\n        return total_hp / total_max if total_max > 0 else 1.0\n    \n    def get_arena_position(self, head_x, head_y):\n        dist_to_edge = min(head_x, head_y, \n                          self.WORLD_COLS - 1 - head_x, \n                          self.WORLD_ROWS - 1 - head_y)\n        max_dist = min(self.WORLD_COLS, self.WORLD_ROWS) / 2\n        return min(dist_to_edge / max_dist, 1.0)\n    \n    def get_threat_density(self, agent_idx, head_x, head_y):\n        threat = 0.0\n        my_length = len(self.agents[agent_idx]['segments'])\n        \n        for i, other in enumerate(self.agents):\n            if i == agent_idx or not other['alive']:\n                continue\n            \n            other_head = other['segments'][0]\n            dist = abs(other_head['x'] - head_x) + abs(other_head['y'] - head_y)\n            \n            if dist < 15:\n                size_factor = 2.0 if len(other['segments']) > my_length else 0.5\n                threat += size_factor / max(dist, 1)\n        \n        return min(threat / 3.0, 1.0)\n    \n    def get_current_gun_type(self, agent):\n        gun_type = agent.get('current_gun', -1)\n        if gun_type >= 0:\n            return (gun_type + 1) * 0.25\n        return 0.0\n    \n    def get_vision_v4(self, agent_idx):\n        \"\"\"\n        96 inputs: 8 dirs × 11 features + 8 state\n        Per direction: [health, mg, sg, cn, pl, self, wall, smaller, bigger, body, enemy_gun_type]\n        State: [health_ratio, arena_pos, threat, length, my_gun, is_mvp, mvp_time, mvp_dist]\n        \"\"\"\n        agent = self.agents[agent_idx]\n        if not agent['alive']:\n            return np.zeros(NEW_INPUT_SIZE, dtype=np.float32)\n        \n        head = agent['segments'][0]\n        head_x, head_y = head['x'], head['y']\n        current_dir = agent['direction']\n        my_length = len(agent['segments'])\n        my_segments = set((s['x'], s['y']) for s in agent['segments'][1:])\n        \n        vision = np.zeros(NEW_INPUT_SIZE, dtype=np.float32)\n        rot = current_dir\n        \n        for ray_idx in range(8):\n            rotated_idx = (ray_idx + rot * 2) % 8\n            dx, dy = self.RAY_OFFSETS[rotated_idx]\n            \n            health_dist = 0.0\n            gun_dists = [0.0, 0.0, 0.0, 0.0]\n            self_danger = wall_dist = 0.0\n            enemy_smaller = enemy_bigger = enemy_body = 0.0\n            nearest_enemy_gun = 0.0\n            nearest_enemy_dist = float('inf')\n            \n            for dist in range(1, 16):\n                cx, cy = head_x + dx * dist, head_y + dy * dist\n                \n                if cx < 0 or cx >= self.WORLD_COLS or cy < 0 or cy >= self.WORLD_ROWS:\n                    if wall_dist == 0:\n                        wall_dist = 1.0 / dist\n                    break\n                \n                if self_danger == 0 and (cx, cy) in my_segments:\n                    self_danger = 1.0 / dist\n                \n                if health_dist == 0 and (cx, cy) in self.health_pickups:\n                    health_dist = 1.0 / dist\n                \n                pos = (cx, cy)\n                if pos in self.gun_pickups:\n                    gun_type = self.gun_pickups[pos]\n                    if gun_dists[gun_type] == 0:\n                        gun_dists[gun_type] = 1.0 / dist\n                \n                if pos in self.segment_map:\n                    other_idx, seg_idx = self.segment_map[pos]\n                    if other_idx != agent_idx and self.agents[other_idx]['alive']:\n                        other = self.agents[other_idx]\n                        other_len = len(other['segments'])\n                        \n                        if seg_idx == 0:  # Head\n                            # Track nearest enemy gun type\n                            if dist < nearest_enemy_dist:\n                                nearest_enemy_dist = dist\n                                gun = other.get('current_gun', 0)\n                                nearest_enemy_gun = (gun + 1) * 0.25\n                            \n                            if other_len < my_length and enemy_smaller == 0:\n                                enemy_smaller = 1.0 / dist\n                            elif enemy_bigger == 0:\n                                enemy_bigger = 1.0 / dist\n                        elif enemy_body == 0:\n                            enemy_body = 1.0 / dist\n            \n            # 11 features per direction\n            base = ray_idx * 11\n            vision[base + 0] = health_dist\n            vision[base + 1] = gun_dists[0]\n            vision[base + 2] = gun_dists[1]\n            vision[base + 3] = gun_dists[2]\n            vision[base + 4] = gun_dists[3]\n            vision[base + 5] = self_danger\n            vision[base + 6] = wall_dist\n            vision[base + 7] = enemy_smaller\n            vision[base + 8] = enemy_bigger\n            vision[base + 9] = enemy_body\n            vision[base + 10] = nearest_enemy_gun\n        \n        # 8 state inputs (88-95)\n        vision[88] = self.get_health_ratio(agent)\n        vision[89] = self.get_arena_position(head_x, head_y)\n        vision[90] = self.get_threat_density(agent_idx, head_x, head_y)\n        vision[91] = min(my_length / 20.0, 1.0)\n        vision[92] = self.get_current_gun_type(agent)\n        vision[93] = 1.0 if agent.get('is_mvp', False) else 0.0\n        vision[94] = min(agent.get('mvp_time', 0) / 500.0, 1.0)  # Normalize to ~500 steps\n        \n        # Distance to MVP\n        mvp_dist = 1.0\n        if self.current_mvp_idx is not None and self.current_mvp_idx != agent_idx:\n            mvp = self.agents[self.current_mvp_idx]\n            if mvp['alive'] and mvp['segments']:\n                mvp_head = mvp['segments'][0]\n                dist = abs(mvp_head['x'] - head_x) + abs(mvp_head['y'] - head_y)\n                mvp_dist = min(dist / 50.0, 1.0)\n        vision[95] = mvp_dist\n        \n        return vision\n    \n    def step(self, actions):\n        self.step_count += 1\n        rewards = np.zeros(self.num_agents, dtype=np.float32)\n        \n        # Apply turns\n        for i, agent in enumerate(self.agents):\n            if not agent['alive']:\n                continue\n            action = actions[i]\n            if action == 0:\n                agent['direction'] = (agent['direction'] - 1) % 4\n            elif action == 2:\n                agent['direction'] = (agent['direction'] + 1) % 4\n        \n        # Move agents\n        for i, agent in enumerate(self.agents):\n            if not agent['alive']:\n                continue\n            \n            old_tail = agent['segments'][-1]\n            agent['prev_tail_pos'] = (old_tail['x'], old_tail['y'])\n            \n            tail_pos = agent['prev_tail_pos']\n            if tail_pos in self.segment_map and self.segment_map[tail_pos][0] == i:\n                del self.segment_map[tail_pos]\n            \n            dx, dy = self.DIRECTIONS[agent['direction']]\n            head = agent['segments'][0]\n            new_head = {\n                'x': head['x'] + dx,\n                'y': head['y'] + dy,\n                'hp': float('inf'),\n                'maxHp': float('inf'),\n                'gun_type': -1\n            }\n            \n            agent['segments'].insert(0, new_head)\n            agent['segments'].pop()\n            \n            for seg_idx, seg in enumerate(agent['segments']):\n                self.segment_map[(seg['x'], seg['y'])] = (i, seg_idx)\n            \n            rewards[i] += 0.01\n        \n        # Check collisions\n        for i, agent in enumerate(self.agents):\n            if not agent['alive']:\n                continue\n            \n            head = agent['segments'][0]\n            hx, hy = head['x'], head['y']\n            \n            if hx < 0 or hx >= self.WORLD_COLS or hy < 0 or hy >= self.WORLD_ROWS:\n                agent['alive'] = False\n                rewards[i] -= 5.0\n                continue\n            \n            my_body = set((s['x'], s['y']) for s in agent['segments'][1:])\n            if (hx, hy) in my_body:\n                agent['alive'] = False\n                rewards[i] -= 5.0\n                continue\n            \n            for j, other in enumerate(self.agents):\n                if i == j or not other['alive']:\n                    continue\n                \n                oh = other['segments'][0]\n                if hx == oh['x'] and hy == oh['y']:\n                    if len(agent['segments']) > len(other['segments']):\n                        other['alive'] = False\n                        rewards[j] -= 5.0\n                        rewards[i] += 3.0\n                    elif len(agent['segments']) < len(other['segments']):\n                        agent['alive'] = False\n                        rewards[i] -= 5.0\n                        rewards[j] += 3.0\n                    else:\n                        agent['alive'] = other['alive'] = False\n                        rewards[i] = rewards[j] = -3.0\n                    break\n                \n                other_body = set((s['x'], s['y']) for s in other['segments'][1:])\n                if (hx, hy) in other_body:\n                    agent['alive'] = False\n                    rewards[i] -= 5.0\n                    rewards[j] += 2.0\n                    break\n        \n        # Pickup collection - \"All Same Gun\" system\n        for i, agent in enumerate(self.agents):\n            if not agent['alive']:\n                continue\n            \n            head = agent['segments'][0]\n            head_pos = (head['x'], head['y'])\n            \n            if head_pos in self.health_pickups:\n                self.health_pickups.remove(head_pos)\n                for seg in agent['segments'][1:]:\n                    if seg['hp'] != float('inf'):\n                        seg['hp'] = min(seg['hp'] + 30, seg['maxHp'])\n                rewards[i] += 0.5\n                self.health_pickups.add((\n                    random.randint(0, self.WORLD_COLS - 1),\n                    random.randint(0, self.WORLD_ROWS - 1)\n                ))\n            \n            if head_pos in self.gun_pickups:\n                gun_type = self.gun_pickups[head_pos]\n                del self.gun_pickups[head_pos]\n                \n                # \"All Same Gun\" - convert ALL segments to new gun type\n                for seg in agent['segments'][1:]:\n                    seg['gun_type'] = gun_type\n                agent['current_gun'] = gun_type\n                \n                # Add new segment\n                prev_tail = agent['prev_tail_pos']\n                agent['segments'].append({\n                    'x': prev_tail[0],\n                    'y': prev_tail[1],\n                    'hp': 100,\n                    'maxHp': 100,\n                    'gun_type': gun_type\n                })\n                self.segment_map[prev_tail] = (i, len(agent['segments']) - 1)\n                agent['score'] += 1\n                rewards[i] += 1.0\n                \n                new_pos = (random.randint(0, self.WORLD_COLS - 1), random.randint(0, self.WORLD_ROWS - 1))\n                self.gun_pickups[new_pos] = random.randint(0, 3)\n        \n        # Update MVP\n        self.update_mvp()\n        \n        # MVP reward bonus\n        for i, agent in enumerate(self.agents):\n            if agent['alive'] and agent.get('is_mvp', False):\n                rewards[i] += 0.02  # Small bonus for being MVP\n        \n        # Random damage\n        if self.step_count % 20 == 0:\n            for agent in self.agents:\n                if agent['alive'] and len(agent['segments']) > 1:\n                    idx = random.randint(1, len(agent['segments']) - 1)\n                    if agent['segments'][idx]['hp'] != float('inf'):\n                        agent['segments'][idx]['hp'] -= random.randint(5, 15)\n                        if agent['segments'][idx]['hp'] <= 0:\n                            agent['segments'].pop(idx)\n        \n        observations = [self.get_vision_v4(i) for i in range(self.num_agents)]\n        dones = [not a['alive'] for a in self.agents]\n        \n        alive = sum(1 for a in self.agents if a['alive'])\n        game_over = alive <= 1 or self.step_count >= 500\n        \n        if game_over and alive == 1:\n            for i, a in enumerate(self.agents):\n                if a['alive']:\n                    rewards[i] += 5.0\n        \n        return observations, rewards.tolist(), dones, game_over",
   "metadata": {
    "id": "environment"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "id": "selfplay_header"
   }
  },
  {
   "cell_type": "code",
   "source": "class SelfPlayTrainer:\n    def __init__(self, pool_size=10, old_brain_path=None):\n        self.current_model = LocomotNetwork(input_size=NEW_INPUT_SIZE).to(device)\n        \n        if old_brain_path and os.path.exists(old_brain_path):\n            # Try to load - transfer if needed\n            with open(old_brain_path, 'r') as f:\n                old_data = json.load(f)\n            old_input_size = len(old_data['net.0.weight'][0]) if 'net.0.weight' in old_data else 0\n            \n            if old_input_size == NEW_INPUT_SIZE:\n                load_existing_weights(self.current_model, old_brain_path)\n                print(f\"✓ Loaded matching {old_input_size}-input brain from {old_brain_path}\")\n            else:\n                transfer_from_old_brain(self.current_model, old_brain_path)\n                print(f\"Transferred from {old_input_size}-input to {NEW_INPUT_SIZE}-input\")\n        else:\n            print(\"Starting with random weights\")\n        \n        self.target_model = LocomotNetwork(input_size=NEW_INPUT_SIZE).to(device)\n        self.target_model.load_state_dict(self.current_model.state_dict())\n        \n        self.opponent_pool = [deepcopy(self.current_model.state_dict())]\n        self.pool_size = pool_size\n        self.memory = deque(maxlen=50000)\n        \n        self.optimizer = optim.Adam(self.current_model.parameters(), lr=0.001)\n        self.gamma = 0.99\n        self.epsilon = 0.5 if old_brain_path else 1.0\n        self.epsilon_min = 0.05\n        self.epsilon_decay = 0.995\n        self.batch_size = 64\n        \n        self.episode_rewards = []\n        self.win_rates = []\n    \n    def train_from_player_data(self, data_path, epochs=10):\n        \"\"\"Imitation learning - train from recorded top player data\"\"\"\n        with open(data_path, 'r') as f:\n            data = json.load(f)\n        \n        if not data:\n            print(\"No player data found\")\n            return\n        \n        # Check input size compatibility\n        sample_state = data[0].get('state', [])\n        if len(sample_state) != NEW_INPUT_SIZE:\n            print(f\"⚠ Player data has {len(sample_state)} inputs, expected {NEW_INPUT_SIZE}\")\n            print(\"  Skipping imitation learning (incompatible data)\")\n            return\n        \n        # Data format: [{state: [...], action: 0/1/2}, ...]\n        states = torch.FloatTensor([d['state'] for d in data]).to(device)\n        actions = torch.LongTensor([d['action'] for d in data]).to(device)\n        \n        print(f\"Training on {len(data)} recorded frames...\")\n        \n        criterion = nn.CrossEntropyLoss()\n        \n        for epoch in tqdm(range(epochs), desc=\"Imitation Learning\"):\n            perm = torch.randperm(len(states))\n            states, actions = states[perm], actions[perm]\n            \n            total_loss = 0\n            correct = 0\n            \n            for i in range(0, len(states), self.batch_size):\n                batch_states = states[i:i+self.batch_size]\n                batch_actions = actions[i:i+self.batch_size]\n                \n                outputs = self.current_model(batch_states)\n                loss = criterion(outputs, batch_actions)\n                \n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n                \n                total_loss += loss.item()\n                correct += (outputs.argmax(1) == batch_actions).sum().item()\n            \n            acc = correct / len(states) * 100\n            tqdm.write(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss:.4f} | Accuracy: {acc:.1f}%\")\n        \n        self.target_model.load_state_dict(self.current_model.state_dict())\n        print(\"Imitation learning complete!\")\n    \n    def select_opponents(self, num_opponents):\n        opponents = []\n        for _ in range(num_opponents):\n            if random.random() < 0.3:\n                opponents.append(deepcopy(self.current_model))\n            else:\n                idx = min(int(random.triangular(0, len(self.opponent_pool), len(self.opponent_pool))), \n                         len(self.opponent_pool) - 1)\n                opponent = LocomotNetwork(input_size=NEW_INPUT_SIZE).to(device)\n                opponent.load_state_dict(self.opponent_pool[idx])\n                opponent.eval()\n                opponents.append(opponent)\n        return opponents\n    \n    def play_episode(self):\n        env = LocomotEnv(num_agents=4)\n        observations = env.reset()\n        opponents = self.select_opponents(3)\n        \n        episode_transitions = []\n        total_reward = 0\n        \n        while True:\n            actions = [self.current_model.get_action(observations[0], self.epsilon)]\n            for i, opp in enumerate(opponents):\n                if env.agents[i + 1]['alive']:\n                    actions.append(opp.get_action(observations[i + 1], 0.0))\n                else:\n                    actions.append(1)\n            \n            next_obs, rewards, dones, game_over = env.step(actions)\n            \n            if env.agents[0]['alive'] or dones[0]:\n                episode_transitions.append((observations[0], actions[0], rewards[0], next_obs[0], dones[0]))\n                total_reward += rewards[0]\n            \n            observations = next_obs\n            if game_over:\n                break\n        \n        for t in episode_transitions:\n            self.memory.append(t)\n        \n        final_health = env.get_health_ratio(env.agents[0]) if env.agents[0]['alive'] else 0\n        return total_reward, env.agents[0]['alive'], len(env.agents[0]['segments']), final_health\n    \n    def train_step(self):\n        if len(self.memory) < self.batch_size:\n            return 0\n        \n        batch = random.sample(self.memory, self.batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n        \n        states = torch.FloatTensor(np.array(states)).to(device)\n        actions = torch.LongTensor(actions).to(device)\n        rewards = torch.FloatTensor(rewards).to(device)\n        next_states = torch.FloatTensor(np.array(next_states)).to(device)\n        dones = torch.FloatTensor(dones).to(device)\n        \n        current_q = self.current_model(states).gather(1, actions.unsqueeze(1))\n        \n        with torch.no_grad():\n            next_actions = self.current_model(next_states).argmax(1)\n            next_q = self.target_model(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n            target_q = rewards + self.gamma * next_q * (1 - dones)\n        \n        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.current_model.parameters(), 1.0)\n        self.optimizer.step()\n        return loss.item()\n    \n    def update_target(self):\n        tau = 0.01\n        for tp, cp in zip(self.target_model.parameters(), self.current_model.parameters()):\n            tp.data.copy_(tau * cp.data + (1 - tau) * tp.data)\n    \n    def add_to_pool(self):\n        self.opponent_pool.append(deepcopy(self.current_model.state_dict()))\n        if len(self.opponent_pool) > self.pool_size:\n            del self.opponent_pool[len(self.opponent_pool) // 2]\n    \n    def train(self, num_episodes=2000, save_every=500):\n        import time\n        wins = 0\n        recent_rewards = deque(maxlen=50)\n        start_time = time.time()\n        \n        pbar = tqdm(range(num_episodes), desc=\"Self-Play Training\")\n        for episode in pbar:\n            reward, won, final_length, final_health = self.play_episode()\n            recent_rewards.append(reward)\n            if won:\n                wins += 1\n            \n            for _ in range(2):\n                self.train_step()\n            \n            self.update_target()\n            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n            \n            if episode % 50 == 0 and episode > 0:\n                self.add_to_pool()\n            \n            if episode % 25 == 0:\n                win_rate = wins / 25 if episode > 0 else 0\n                avg_reward = np.mean(recent_rewards) if recent_rewards else 0\n                pbar.set_postfix({\n                    'R': f'{avg_reward:.1f}',\n                    'Win': f'{win_rate:.0%}',\n                    'ε': f'{self.epsilon:.2f}'\n                })\n                self.episode_rewards.append(avg_reward)\n                self.win_rates.append(win_rate)\n                wins = 0\n            \n            if episode % save_every == 0 and episode > 0:\n                save_brain_json(self.current_model, f'brain_v3_ep{episode}.json')\n        \n        elapsed = time.time() - start_time\n        print(f'\\nDone! {elapsed:.1f}s ({num_episodes/elapsed:.1f} ep/s)')\n        return self.current_model",
   "metadata": {
    "id": "selfplay"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Training\nRun the cell below to start training. Takes ~5-10 minutes.",
   "metadata": {
    "id": "run_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Auto-find or fetch brain file\nimport glob\nimport urllib.request\nfrom datetime import datetime\n\n# GitHub raw URL for the brain file\nBRAIN_URL = 'https://raw.githubusercontent.com/savecharlie/locomot-io/main/training/brain_85_init.json'\nSERVER_URL = 'https://locomot-io.savecharlie.partykit.dev'\n\ndef find_local_brain():\n    \"\"\"Find any brain.json file locally\"\"\"\n    search_paths = ['.', '/content']\n    all_brains = []\n    \n    for path in search_paths:\n        if os.path.exists(path):\n            for f in glob.glob(os.path.join(path, '*.json')):\n                if 'brain' in os.path.basename(f).lower():\n                    all_brains.append(f)\n    \n    if all_brains:\n        all_brains.sort(key=lambda f: os.path.getmtime(f), reverse=True)\n        return all_brains[0]\n    return None\n\ndef fetch_brain_from_github():\n    \"\"\"Fetch brain from GitHub\"\"\"\n    try:\n        print(f\"Fetching from {BRAIN_URL}...\")\n        with urllib.request.urlopen(BRAIN_URL, timeout=30) as response:\n            brain_data = json.loads(response.read().decode())\n            if brain_data and 'net.0.weight' in brain_data:\n                with open('brain_from_github.json', 'w') as f:\n                    json.dump(brain_data, f)\n                input_size = len(brain_data['net.0.weight'][0])\n                print(f\"✓ Fetched brain from GitHub ({input_size} inputs)\")\n                return 'brain_from_github.json'\n    except Exception as e:\n        print(f\"Could not fetch brain from GitHub: {e}\")\n    return None\n\n# Try to find brain: local first, then GitHub\nbrain_path = find_local_brain()\n\nif brain_path:\n    print(f\"✓ Found local brain: {brain_path}\")\nelse:\n    print(\"No local brain found, fetching from GitHub...\")\n    brain_path = fetch_brain_from_github()\n\nif not brain_path:\n    print(\"⚠ No brain found - starting fresh with random weights\")\n\n# Fetch player training data from server\nprint(\"\\nFetching player training data from server...\")\ntry:\n    req = urllib.request.Request(\n        f'{SERVER_URL}/party/collective',\n        data=json.dumps({'type': 'get_player_data'}).encode(),\n        headers={'Content-Type': 'application/json'},\n        method='POST'\n    )\n    with urllib.request.urlopen(req, timeout=10) as response:\n        player_data = json.loads(response.read().decode())\n        if player_data and len(player_data) > 0:\n            with open('player_data.json', 'w') as f:\n                json.dump(player_data, f)\n            print(f\"✓ Got {len(player_data)} frames from top players!\")\n            has_player_data = True\n        else:\n            print(\"No player data on server yet\")\n            has_player_data = False\nexcept Exception as e:\n    print(f\"Could not fetch player data: {e}\")\n    has_player_data = False\n\n# Train!\ntrainer = SelfPlayTrainer(pool_size=10, old_brain_path=brain_path)\n\n# If we have player data, do imitation learning first\nif has_player_data:\n    print(\"\\n=== Imitation Learning from Top Players ===\")\n    trainer.train_from_player_data('player_data.json', epochs=10)\n\n# Then self-play\nprint(\"\\n=== Self-Play Training ===\")\ntrained_model = trainer.train(num_episodes=2000, save_every=500)",
   "metadata": {
    "id": "run_training"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot training progress\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(trainer.episode_rewards)\n",
    "ax1.set_title('Average Episode Reward')\n",
    "ax1.set_xlabel('Episode (×100)')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(trainer.win_rates)\n",
    "ax2.set_title('Win Rate vs Self-Play Pool')\n",
    "ax2.set_xlabel('Episode (×100)')\n",
    "ax2.set_ylabel('Win Rate')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "plot"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save with timestamp so next run auto-finds it\nfrom datetime import datetime\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\nfilename = f'brain_{timestamp}.json'\n\nsave_brain_json(trained_model, filename)\nprint(f\"\\n✓ Saved as {filename}\")\nprint(\"Next run will auto-load this brain!\")\n\nif IN_COLAB:\n    files.download(filename)\n    print('Downloaded! Update index.html with this brain.')",
   "metadata": {
    "id": "save"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Architecture v3 (85 inputs)\n\nAfter training completes, the brain will work with the updated `index.html`.\n\n**New format (85 inputs = 8 dirs × 10 features + 5 state):**\n\nPer direction (10 features):\n- health_pickup - distance to health pickup\n- machinegun_pickup - distance to MACHINEGUN pickup\n- shotgun_pickup - distance to SHOTGUN pickup\n- cannon_pickup - distance to CANNON pickup\n- pulse_pickup - distance to PULSE pickup\n- self - distance to own body\n- wall - distance to wall/edge\n- smaller_head - distance to smaller enemy head\n- bigger_head - distance to bigger enemy head\n- enemy_body - distance to enemy body\n\nState inputs (5):\n- health_ratio - overall body health (0-1)\n- arena_position - distance from edge (0-1)\n- threat_density - nearby enemy threat (0-1)\n- my_length - normalized snake length (0-1)\n- current_gun_type - what gun first body segment has (0-1)\n\n**Changes from v2:**\n- Gun pickup split from 1 to 4 inputs (per gun type)\n- Added current_gun_type state input\n- Hidden layer expanded (64 → 128) for more features\n- Pickup cooloff: enemies can't pick up own recent drops (3 sec cooloff)",
   "metadata": {
    "id": "howto"
   }
  }
 ]
}