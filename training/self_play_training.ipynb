{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# LOCOMOT.IO Neural Network Training v2\n\n## Quick Start (Works in Colab!)\n1. Runtime → Run all\n2. Upload `old_brain.json` when prompted\n3. Wait ~5-10 min for training\n4. Download `brain_v2_final.json` when done\n\n**Two training modes:**\n- **Self-play**: AI learns by playing against itself\n- **Imitation**: AI learns from recorded top player data",
   "metadata": {
    "id": "intro"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": "# Setup - works in Colab or locally\nimport subprocess\nimport sys\n\n# Install dependencies if needed (Colab)\ntry:\n    import torch\n    from tqdm.auto import tqdm\nexcept ImportError:\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'torch', 'numpy', 'matplotlib', 'tqdm', '-q'])\n    from tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nimport json\nfrom collections import deque\nfrom copy import deepcopy\nimport matplotlib.pyplot as plt\nimport os\nfrom tqdm.auto import tqdm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using: {device}')\n\n# Handle file uploads for Colab\nIN_COLAB = 'google.colab' in sys.modules\nif IN_COLAB:\n    from google.colab import files\n    print('Running in Colab - will prompt for file uploads')\nelse:\n    print('Running locally')"
  },
  {
   "cell_type": "markdown",
   "source": "## Setup (just run these cells)",
   "metadata": {
    "id": "nn_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Architecture constants\nOLD_INPUT_SIZE = 48  # 8 directions × 6 features\nNEW_INPUT_SIZE = 60  # 8 directions × 7 features + 4 state inputs\nHIDDEN_SIZE = 64\nOUTPUT_SIZE = 3\n\nclass LocomotNetwork(nn.Module):\n    def __init__(self, input_size=NEW_INPUT_SIZE):\n        super().__init__()\n        self.input_size = input_size\n        self.net = nn.Sequential(\n            nn.Linear(input_size, HIDDEN_SIZE),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n    \n    def get_action(self, state, epsilon=0.0):\n        \"\"\"Get action with epsilon-greedy exploration\"\"\"\n        if random.random() < epsilon:\n            return random.randint(0, 2)\n        with torch.no_grad():\n            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n            q_values = self.forward(state_t)\n            return q_values.argmax(dim=1).item()\n\n\ndef load_existing_weights(model, json_path):\n    \"\"\"Load weights from JavaScript brain.json format (same input size)\"\"\"\n    with open(json_path, 'r') as f:\n        weights = json.load(f)\n    \n    state_dict = {\n        'net.0.weight': torch.FloatTensor(weights['net.0.weight']),\n        'net.0.bias': torch.FloatTensor(weights['net.0.bias']),\n        'net.2.weight': torch.FloatTensor(weights['net.2.weight']),\n        'net.2.bias': torch.FloatTensor(weights['net.2.bias']),\n        'net.4.weight': torch.FloatTensor(weights['net.4.weight']),\n        'net.4.bias': torch.FloatTensor(weights['net.4.bias']),\n    }\n    model.load_state_dict(state_dict)\n    return model\n\n\ndef transfer_from_old_brain(new_model, old_json_path):\n    \"\"\"\n    Transfer learning: Load old 48-input brain weights into new 60-input model.\n    \n    Old format (48 inputs = 8 dirs × 6 features):\n      Per direction: [food, self, wall, smaller_head, bigger_head, enemy_body]\n    \n    New format (60 inputs = 8 dirs × 7 features + 4 state):\n      Per direction: [health_pickup, gun_pickup, self, wall, smaller_head, bigger_head, enemy_body]\n      State: [health_ratio, arena_position, threat_density, my_length]\n    \n    Mapping strategy:\n    - Old 'food' weights → average into both health_pickup and gun_pickup\n    - Old self/wall/enemy weights → copy directly to new positions\n    - New state inputs → initialize with small random weights\n    \"\"\"\n    with open(old_json_path, 'r') as f:\n        old_weights = json.load(f)\n    \n    old_w0 = torch.FloatTensor(old_weights['net.0.weight'])  # [64, 48]\n    old_b0 = torch.FloatTensor(old_weights['net.0.bias'])    # [64]\n    \n    # Create new first layer weights [64, 60]\n    new_w0 = torch.zeros(HIDDEN_SIZE, NEW_INPUT_SIZE)\n    \n    # Map old weights to new positions\n    # Old: 8 dirs × [food, self, wall, smaller, bigger, body] = indices 0-47\n    # New: 8 dirs × [health, gun, self, wall, smaller, bigger, body] + 4 state = indices 0-59\n    \n    for d in range(8):  # 8 directions\n        old_base = d * 6  # Old: 6 features per direction\n        new_base = d * 7  # New: 7 features per direction\n        \n        # Old food → split to health_pickup and gun_pickup (indices 0, 1)\n        food_weights = old_w0[:, old_base + 0]\n        new_w0[:, new_base + 0] = food_weights  # health_pickup\n        new_w0[:, new_base + 1] = food_weights  # gun_pickup (same initially)\n        \n        # self (1→2), wall (2→3), smaller (3→4), bigger (4→5), body (5→6)\n        new_w0[:, new_base + 2] = old_w0[:, old_base + 1]  # self\n        new_w0[:, new_base + 3] = old_w0[:, old_base + 2]  # wall\n        new_w0[:, new_base + 4] = old_w0[:, old_base + 3]  # smaller_head\n        new_w0[:, new_base + 5] = old_w0[:, old_base + 4]  # bigger_head\n        new_w0[:, new_base + 6] = old_w0[:, old_base + 5]  # enemy_body\n    \n    # Initialize new state inputs (indices 56-59) with small random weights\n    # These will be learned during training\n    nn.init.xavier_uniform_(new_w0[:, 56:60])\n    \n    # Load into new model\n    new_state_dict = new_model.state_dict()\n    new_state_dict['net.0.weight'] = new_w0\n    new_state_dict['net.0.bias'] = old_b0  # Bias stays same size\n    \n    # Copy remaining layers directly (they stay the same size)\n    new_state_dict['net.2.weight'] = torch.FloatTensor(old_weights['net.2.weight'])\n    new_state_dict['net.2.bias'] = torch.FloatTensor(old_weights['net.2.bias'])\n    new_state_dict['net.4.weight'] = torch.FloatTensor(old_weights['net.4.weight'])\n    new_state_dict['net.4.bias'] = torch.FloatTensor(old_weights['net.4.bias'])\n    \n    new_model.load_state_dict(new_state_dict)\n    print(f\"✓ Transferred weights from old 48-input brain to new 60-input model\")\n    print(f\"  - Spatial features mapped (food → health+gun)\")\n    print(f\"  - State inputs initialized with Xavier uniform\")\n    return new_model\n\n\ndef save_brain_json(model, path):\n    \"\"\"Export weights to JavaScript-compatible format\"\"\"\n    state_dict = model.state_dict()\n    brain = {\n        'input_size': model.input_size,  # Store input size for compatibility checking\n        'net.0.weight': state_dict['net.0.weight'].cpu().numpy().tolist(),\n        'net.0.bias': state_dict['net.0.bias'].cpu().numpy().tolist(),\n        'net.2.weight': state_dict['net.2.weight'].cpu().numpy().tolist(),\n        'net.2.bias': state_dict['net.2.bias'].cpu().numpy().tolist(),\n        'net.4.weight': state_dict['net.4.weight'].cpu().numpy().tolist(),\n        'net.4.bias': state_dict['net.4.bias'].cpu().numpy().tolist(),\n    }\n    with open(path, 'w') as f:\n        json.dump(brain, f)\n    print(f'Saved brain to {path} (input_size={model.input_size})')",
   "metadata": {
    "id": "network"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "id": "env_header"
   }
  },
  {
   "cell_type": "code",
   "source": "class LocomotEnv:\n    \"\"\"FAST simplified LOCOMOT.IO environment for self-play with extended inputs\"\"\"\n    \n    WORLD_COLS = 50  # Smaller world = faster\n    WORLD_ROWS = 40\n    DIRECTIONS = [(0, -1), (1, 0), (0, 1), (-1, 0)]  # UP, RIGHT, DOWN, LEFT\n    \n    # Pre-compute direction offsets for 8 rays\n    RAY_OFFSETS = [\n        (0, -1), (1, -1), (1, 0), (1, 1),\n        (0, 1), (-1, 1), (-1, 0), (-1, -1)\n    ]\n    \n    def __init__(self, num_agents=4):\n        self.num_agents = num_agents\n        self.reset()\n    \n    def reset(self):\n        self.agents = []\n        self.health_pickups = set()  # Health pickups (green)\n        self.gun_pickups = set()     # Gun pickups (various colors)\n        self.segment_map = {}  # (x,y) -> (agent_idx, seg_idx) for fast collision\n        self.step_count = 0\n        \n        # Spawn agents with health tracking\n        for i in range(self.num_agents):\n            x = random.randint(8, self.WORLD_COLS - 8)\n            y = random.randint(8, self.WORLD_ROWS - 8)\n            direction = random.randint(0, 3)\n            dx, dy = self.DIRECTIONS[direction]\n            \n            # Each segment has hp/maxHp (head is invincible)\n            segments = []\n            for j in range(4):\n                seg = {\n                    'x': x - j*dx,\n                    'y': y - j*dy,\n                    'hp': 100 if j > 0 else float('inf'),  # Head invincible\n                    'maxHp': 100 if j > 0 else float('inf')\n                }\n                segments.append(seg)\n            \n            self.agents.append({\n                'segments': segments,\n                'direction': direction,\n                'alive': True,\n                'score': 0,\n                'prev_tail_pos': None  # Track previous tail position for pickup growth\n            })\n            \n            # Add to segment map\n            for seg_idx, seg in enumerate(segments):\n                self.segment_map[(seg['x'], seg['y'])] = (i, seg_idx)\n        \n        # Spawn pickups (70% gun, 30% health like the real game)\n        for _ in range(15):\n            self.gun_pickups.add((\n                random.randint(0, self.WORLD_COLS - 1),\n                random.randint(0, self.WORLD_ROWS - 1)\n            ))\n        for _ in range(5):\n            self.health_pickups.add((\n                random.randint(0, self.WORLD_COLS - 1),\n                random.randint(0, self.WORLD_ROWS - 1)\n            ))\n        \n        return [self.get_vision_v2(i) for i in range(self.num_agents)]\n    \n    def get_health_ratio(self, agent):\n        \"\"\"Calculate health ratio (0-1) from body segments\"\"\"\n        total_hp = 0\n        total_max = 0\n        for seg in agent['segments'][1:]:  # Skip head (infinite hp)\n            if seg['hp'] != float('inf'):\n                total_hp += seg['hp']\n                total_max += seg['maxHp']\n        return total_hp / total_max if total_max > 0 else 1.0\n    \n    def get_arena_position(self, head_x, head_y):\n        \"\"\"Calculate arena position safety (0-1, higher = more centered)\"\"\"\n        dist_to_edge = min(head_x, head_y, \n                          self.WORLD_COLS - 1 - head_x, \n                          self.WORLD_ROWS - 1 - head_y)\n        max_dist = min(self.WORLD_COLS, self.WORLD_ROWS) / 2\n        return min(dist_to_edge / max_dist, 1.0)\n    \n    def get_threat_density(self, agent_idx, head_x, head_y):\n        \"\"\"Calculate threat density from nearby enemies (0-1)\"\"\"\n        threat = 0.0\n        my_length = len(self.agents[agent_idx]['segments'])\n        \n        for i, other in enumerate(self.agents):\n            if i == agent_idx or not other['alive']:\n                continue\n            \n            other_head = other['segments'][0]\n            dist = abs(other_head['x'] - head_x) + abs(other_head['y'] - head_y)\n            \n            if dist < 15:\n                # Bigger enemies are more threatening\n                size_factor = 2.0 if len(other['segments']) > my_length else 0.5\n                threat += size_factor / max(dist, 1)\n        \n        # Normalize to 0-1 (cap at reasonable max)\n        return min(threat / 3.0, 1.0)\n    \n    def get_vision_v2(self, agent_idx):\n        \"\"\"\n        Extended vision with 60 inputs:\n        - 56 spatial: 8 directions × 7 features\n          [health_pickup, gun_pickup, self, wall, smaller_head, bigger_head, enemy_body]\n        - 4 state: [health_ratio, arena_position, threat_density, my_length_normalized]\n        \"\"\"\n        agent = self.agents[agent_idx]\n        if not agent['alive']:\n            return np.zeros(NEW_INPUT_SIZE, dtype=np.float32)\n        \n        head = agent['segments'][0]\n        head_x, head_y = head['x'], head['y']\n        current_dir = agent['direction']\n        my_length = len(agent['segments'])\n        my_segments = set((s['x'], s['y']) for s in agent['segments'][1:])\n        \n        vision = np.zeros(NEW_INPUT_SIZE, dtype=np.float32)\n        \n        # Rotate ray offsets based on direction\n        rot = current_dir\n        \n        for ray_idx in range(8):\n            rotated_idx = (ray_idx + rot * 2) % 8\n            dx, dy = self.RAY_OFFSETS[rotated_idx]\n            \n            health_dist = gun_dist = self_danger = wall_dist = 0.0\n            enemy_smaller = enemy_bigger = enemy_body = 0.0\n            \n            for dist in range(1, 16):\n                cx, cy = head_x + dx * dist, head_y + dy * dist\n                \n                # Wall\n                if cx < 0 or cx >= self.WORLD_COLS or cy < 0 or cy >= self.WORLD_ROWS:\n                    if wall_dist == 0:\n                        wall_dist = 1.0 / dist\n                    break\n                \n                # Self body\n                if self_danger == 0 and (cx, cy) in my_segments:\n                    self_danger = 1.0 / dist\n                \n                # Health pickup\n                if health_dist == 0 and (cx, cy) in self.health_pickups:\n                    health_dist = 1.0 / dist\n                \n                # Gun pickup\n                if gun_dist == 0 and (cx, cy) in self.gun_pickups:\n                    gun_dist = 1.0 / dist\n                \n                # Other agents\n                pos = (cx, cy)\n                if pos in self.segment_map:\n                    other_idx, seg_idx = self.segment_map[pos]\n                    if other_idx != agent_idx and self.agents[other_idx]['alive']:\n                        other_len = len(self.agents[other_idx]['segments'])\n                        if seg_idx == 0:  # Head\n                            if other_len < my_length and enemy_smaller == 0:\n                                enemy_smaller = 1.0 / dist\n                            elif enemy_bigger == 0:\n                                enemy_bigger = 1.0 / dist\n                        elif enemy_body == 0:\n                            enemy_body = 1.0 / dist\n            \n            # Store 7 features per direction\n            base = ray_idx * 7\n            vision[base:base+7] = [health_dist, gun_dist, self_danger, wall_dist, \n                                   enemy_smaller, enemy_bigger, enemy_body]\n        \n        # Add 4 state inputs (indices 56-59)\n        vision[56] = self.get_health_ratio(agent)\n        vision[57] = self.get_arena_position(head_x, head_y)\n        vision[58] = self.get_threat_density(agent_idx, head_x, head_y)\n        vision[59] = min(my_length / 20.0, 1.0)  # Normalized length (cap at 20)\n        \n        return vision\n    \n    def step(self, actions):\n        \"\"\"Execute actions. Returns (observations, rewards, dones, game_over)\"\"\"\n        self.step_count += 1\n        rewards = np.zeros(self.num_agents, dtype=np.float32)\n        \n        # Apply turns\n        for i, agent in enumerate(self.agents):\n            if not agent['alive']:\n                continue\n            action = actions[i]\n            if action == 0:\n                agent['direction'] = (agent['direction'] - 1) % 4\n            elif action == 2:\n                agent['direction'] = (agent['direction'] + 1) % 4\n        \n        # Move agents and update segment map\n        for i, agent in enumerate(self.agents):\n            if not agent['alive']:\n                continue\n            \n            # Save previous tail position BEFORE movement (for pickup growth)\n            old_tail = agent['segments'][-1]\n            agent['prev_tail_pos'] = (old_tail['x'], old_tail['y'])\n            \n            # Remove old tail from map\n            tail_pos = agent['prev_tail_pos']\n            if tail_pos in self.segment_map and self.segment_map[tail_pos][0] == i:\n                del self.segment_map[tail_pos]\n            \n            dx, dy = self.DIRECTIONS[agent['direction']]\n            head = agent['segments'][0]\n            new_head = {\n                'x': head['x'] + dx,\n                'y': head['y'] + dy,\n                'hp': float('inf'),\n                'maxHp': float('inf')\n            }\n            \n            agent['segments'].insert(0, new_head)\n            agent['segments'].pop()\n            \n            # Update segment map\n            for seg_idx, seg in enumerate(agent['segments']):\n                self.segment_map[(seg['x'], seg['y'])] = (i, seg_idx)\n            \n            rewards[i] += 0.01\n        \n        # Check collisions\n        for i, agent in enumerate(self.agents):\n            if not agent['alive']:\n                continue\n            \n            head = agent['segments'][0]\n            hx, hy = head['x'], head['y']\n            \n            # Wall\n            if hx < 0 or hx >= self.WORLD_COLS or hy < 0 or hy >= self.WORLD_ROWS:\n                agent['alive'] = False\n                rewards[i] -= 5.0\n                continue\n            \n            # Self collision\n            my_body = set((s['x'], s['y']) for s in agent['segments'][1:])\n            if (hx, hy) in my_body:\n                agent['alive'] = False\n                rewards[i] -= 5.0\n                continue\n            \n            # Enemy collision\n            for j, other in enumerate(self.agents):\n                if i == j or not other['alive']:\n                    continue\n                \n                oh = other['segments'][0]\n                if hx == oh['x'] and hy == oh['y']:\n                    # Head-to-head\n                    if len(agent['segments']) > len(other['segments']):\n                        other['alive'] = False\n                        rewards[j] -= 5.0\n                        rewards[i] += 3.0\n                    elif len(agent['segments']) < len(other['segments']):\n                        agent['alive'] = False\n                        rewards[i] -= 5.0\n                        rewards[j] += 3.0\n                    else:\n                        agent['alive'] = other['alive'] = False\n                        rewards[i] = rewards[j] = -3.0\n                    break\n                \n                # Head into body\n                other_body = set((s['x'], s['y']) for s in other['segments'][1:])\n                if (hx, hy) in other_body:\n                    agent['alive'] = False\n                    rewards[i] -= 5.0\n                    rewards[j] += 2.0\n                    break\n        \n        # Pickup collection\n        for i, agent in enumerate(self.agents):\n            if not agent['alive']:\n                continue\n            \n            head = agent['segments'][0]\n            head_pos = (head['x'], head['y'])\n            \n            # Health pickup - heals segments\n            if head_pos in self.health_pickups:\n                self.health_pickups.remove(head_pos)\n                # Heal all body segments\n                for seg in agent['segments'][1:]:\n                    if seg['hp'] != float('inf'):\n                        seg['hp'] = min(seg['hp'] + 30, seg['maxHp'])\n                rewards[i] += 0.5\n                # Respawn\n                self.health_pickups.add((\n                    random.randint(0, self.WORLD_COLS - 1),\n                    random.randint(0, self.WORLD_ROWS - 1)\n                ))\n            \n            # Gun pickup - grows snake (use PREVIOUS tail position to avoid self-collision)\n            if head_pos in self.gun_pickups:\n                self.gun_pickups.remove(head_pos)\n                prev_tail = agent['prev_tail_pos']\n                agent['segments'].append({\n                    'x': prev_tail[0],\n                    'y': prev_tail[1],\n                    'hp': 100,\n                    'maxHp': 100\n                })\n                # Update segment map for new segment\n                self.segment_map[prev_tail] = (i, len(agent['segments']) - 1)\n                agent['score'] += 1\n                rewards[i] += 1.0\n                # Respawn\n                self.gun_pickups.add((\n                    random.randint(0, self.WORLD_COLS - 1),\n                    random.randint(0, self.WORLD_ROWS - 1)\n                ))\n        \n        # Random damage to simulate combat\n        if self.step_count % 20 == 0:\n            for agent in self.agents:\n                if agent['alive'] and len(agent['segments']) > 1:\n                    # Random segment takes minor damage\n                    idx = random.randint(1, len(agent['segments']) - 1)\n                    if agent['segments'][idx]['hp'] != float('inf'):\n                        agent['segments'][idx]['hp'] -= random.randint(5, 15)\n                        if agent['segments'][idx]['hp'] <= 0:\n                            # Segment destroyed - snake shrinks\n                            agent['segments'].pop(idx)\n        \n        observations = [self.get_vision_v2(i) for i in range(self.num_agents)]\n        dones = [not a['alive'] for a in self.agents]\n        \n        alive = sum(1 for a in self.agents if a['alive'])\n        game_over = alive <= 1 or self.step_count >= 500\n        \n        if game_over and alive == 1:\n            for i, a in enumerate(self.agents):\n                if a['alive']:\n                    rewards[i] += 5.0\n        \n        return observations, rewards.tolist(), dones, game_over",
   "metadata": {
    "id": "environment"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "id": "selfplay_header"
   }
  },
  {
   "cell_type": "code",
   "source": "class SelfPlayTrainer:\n    def __init__(self, pool_size=10, old_brain_path=None):\n        self.current_model = LocomotNetwork(input_size=NEW_INPUT_SIZE).to(device)\n        \n        if old_brain_path and os.path.exists(old_brain_path):\n            transfer_from_old_brain(self.current_model, old_brain_path)\n            print(f\"Loaded weights from {old_brain_path}\")\n        else:\n            print(\"Starting with random weights\")\n        \n        self.target_model = LocomotNetwork(input_size=NEW_INPUT_SIZE).to(device)\n        self.target_model.load_state_dict(self.current_model.state_dict())\n        \n        self.opponent_pool = [deepcopy(self.current_model.state_dict())]\n        self.pool_size = pool_size\n        self.memory = deque(maxlen=50000)\n        \n        self.optimizer = optim.Adam(self.current_model.parameters(), lr=0.001)\n        self.gamma = 0.99\n        self.epsilon = 0.5 if old_brain_path else 1.0\n        self.epsilon_min = 0.05\n        self.epsilon_decay = 0.995\n        self.batch_size = 64\n        \n        self.episode_rewards = []\n        self.win_rates = []\n    \n    def train_from_player_data(self, data_path, epochs=10):\n        \"\"\"Imitation learning - train from recorded top player data\"\"\"\n        with open(data_path, 'r') as f:\n            data = json.load(f)\n        \n        if not data:\n            print(\"No player data found\")\n            return\n        \n        # Data format: [{state: [...], action: 0/1/2}, ...]\n        states = torch.FloatTensor([d['state'] for d in data]).to(device)\n        actions = torch.LongTensor([d['action'] for d in data]).to(device)\n        \n        print(f\"Training on {len(data)} recorded frames...\")\n        \n        criterion = nn.CrossEntropyLoss()\n        \n        for epoch in tqdm(range(epochs), desc=\"Imitation Learning\"):\n            # Shuffle\n            perm = torch.randperm(len(states))\n            states, actions = states[perm], actions[perm]\n            \n            total_loss = 0\n            correct = 0\n            \n            for i in range(0, len(states), self.batch_size):\n                batch_states = states[i:i+self.batch_size]\n                batch_actions = actions[i:i+self.batch_size]\n                \n                outputs = self.current_model(batch_states)\n                loss = criterion(outputs, batch_actions)\n                \n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n                \n                total_loss += loss.item()\n                correct += (outputs.argmax(1) == batch_actions).sum().item()\n            \n            acc = correct / len(states) * 100\n            tqdm.write(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss:.4f} | Accuracy: {acc:.1f}%\")\n        \n        # Update target\n        self.target_model.load_state_dict(self.current_model.state_dict())\n        print(\"Imitation learning complete!\")\n    \n    def select_opponents(self, num_opponents):\n        opponents = []\n        for _ in range(num_opponents):\n            if random.random() < 0.3:\n                opponents.append(deepcopy(self.current_model))\n            else:\n                idx = min(int(random.triangular(0, len(self.opponent_pool), len(self.opponent_pool))), \n                         len(self.opponent_pool) - 1)\n                opponent = LocomotNetwork(input_size=NEW_INPUT_SIZE).to(device)\n                opponent.load_state_dict(self.opponent_pool[idx])\n                opponent.eval()\n                opponents.append(opponent)\n        return opponents\n    \n    def play_episode(self):\n        env = LocomotEnv(num_agents=4)\n        observations = env.reset()\n        opponents = self.select_opponents(3)\n        \n        episode_transitions = []\n        total_reward = 0\n        \n        while True:\n            actions = [self.current_model.get_action(observations[0], self.epsilon)]\n            for i, opp in enumerate(opponents):\n                if env.agents[i + 1]['alive']:\n                    actions.append(opp.get_action(observations[i + 1], 0.0))\n                else:\n                    actions.append(1)\n            \n            next_obs, rewards, dones, game_over = env.step(actions)\n            \n            if env.agents[0]['alive'] or dones[0]:\n                episode_transitions.append((observations[0], actions[0], rewards[0], next_obs[0], dones[0]))\n                total_reward += rewards[0]\n            \n            observations = next_obs\n            if game_over:\n                break\n        \n        for t in episode_transitions:\n            self.memory.append(t)\n        \n        final_health = env.get_health_ratio(env.agents[0]) if env.agents[0]['alive'] else 0\n        return total_reward, env.agents[0]['alive'], len(env.agents[0]['segments']), final_health\n    \n    def train_step(self):\n        if len(self.memory) < self.batch_size:\n            return 0\n        \n        batch = random.sample(self.memory, self.batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n        \n        states = torch.FloatTensor(np.array(states)).to(device)\n        actions = torch.LongTensor(actions).to(device)\n        rewards = torch.FloatTensor(rewards).to(device)\n        next_states = torch.FloatTensor(np.array(next_states)).to(device)\n        dones = torch.FloatTensor(dones).to(device)\n        \n        current_q = self.current_model(states).gather(1, actions.unsqueeze(1))\n        \n        with torch.no_grad():\n            next_actions = self.current_model(next_states).argmax(1)\n            next_q = self.target_model(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n            target_q = rewards + self.gamma * next_q * (1 - dones)\n        \n        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.current_model.parameters(), 1.0)\n        self.optimizer.step()\n        return loss.item()\n    \n    def update_target(self):\n        tau = 0.01\n        for tp, cp in zip(self.target_model.parameters(), self.current_model.parameters()):\n            tp.data.copy_(tau * cp.data + (1 - tau) * tp.data)\n    \n    def add_to_pool(self):\n        self.opponent_pool.append(deepcopy(self.current_model.state_dict()))\n        if len(self.opponent_pool) > self.pool_size:\n            del self.opponent_pool[len(self.opponent_pool) // 2]\n    \n    def train(self, num_episodes=2000, save_every=500):\n        import time\n        wins = 0\n        recent_rewards = deque(maxlen=50)\n        start_time = time.time()\n        \n        pbar = tqdm(range(num_episodes), desc=\"Self-Play Training\")\n        for episode in pbar:\n            reward, won, final_length, final_health = self.play_episode()\n            recent_rewards.append(reward)\n            if won:\n                wins += 1\n            \n            for _ in range(2):\n                self.train_step()\n            \n            self.update_target()\n            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n            \n            if episode % 50 == 0 and episode > 0:\n                self.add_to_pool()\n            \n            if episode % 25 == 0:\n                win_rate = wins / 25 if episode > 0 else 0\n                avg_reward = np.mean(recent_rewards) if recent_rewards else 0\n                pbar.set_postfix({\n                    'R': f'{avg_reward:.1f}',\n                    'Win': f'{win_rate:.0%}',\n                    'ε': f'{self.epsilon:.2f}'\n                })\n                self.episode_rewards.append(avg_reward)\n                self.win_rates.append(win_rate)\n                wins = 0\n            \n            if episode % save_every == 0 and episode > 0:\n                save_brain_json(self.current_model, f'brain_v2_ep{episode}.json')\n        \n        elapsed = time.time() - start_time\n        print(f'\\nDone! {elapsed:.1f}s ({num_episodes/elapsed:.1f} ep/s)')\n        return self.current_model",
   "metadata": {
    "id": "selfplay"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Training\nRun the cell below to start training. Takes ~5-10 minutes.",
   "metadata": {
    "id": "run_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# Auto-find newest brain file or upload\nimport glob\nfrom datetime import datetime\n\ndef find_newest_brain():\n    \"\"\"Find the newest brain file by timestamp in filename\"\"\"\n    patterns = ['brain_*.json', 'old_brain*.json']\n    all_brains = []\n    for p in patterns:\n        all_brains.extend(glob.glob(p))\n    \n    if not all_brains:\n        return None\n    \n    # Sort by modification time, newest first\n    all_brains.sort(key=lambda f: os.path.getmtime(f), reverse=True)\n    return all_brains[0]\n\n# Try to find existing brain\nbrain_path = find_newest_brain()\n\nif brain_path:\n    print(f\"✓ Found brain: {brain_path}\")\nelif IN_COLAB:\n    print(\"No brain found. Upload one:\")\n    uploaded = files.upload()\n    brain_files = [f for f in uploaded.keys() if f.endswith('.json')]\n    if brain_files:\n        brain_path = brain_files[0]\n        print(f\"✓ Using uploaded: {brain_path}\")\n    else:\n        print(\"⚠ No brain uploaded - starting fresh\")\n        brain_path = None\nelse:\n    print(\"⚠ No brain found - starting fresh\")\n\n# Fetch player training data from server\nimport urllib.request\nprint(\"\\nFetching player training data from server...\")\ntry:\n    req = urllib.request.Request(\n        'https://locomot-io.savecharlie.partykit.dev/party/collective',\n        data=json.dumps({'type': 'get_player_data'}).encode(),\n        headers={'Content-Type': 'application/json'},\n        method='POST'\n    )\n    with urllib.request.urlopen(req, timeout=10) as response:\n        player_data = json.loads(response.read().decode())\n        if player_data and len(player_data) > 0:\n            with open('player_data.json', 'w') as f:\n                json.dump(player_data, f)\n            print(f\"✓ Got {len(player_data)} frames from top players!\")\n            has_player_data = True\n        else:\n            print(\"No player data on server yet\")\n            has_player_data = False\nexcept Exception as e:\n    print(f\"Could not fetch player data: {e}\")\n    has_player_data = False\n\n# Train!\ntrainer = SelfPlayTrainer(pool_size=10, old_brain_path=brain_path)\n\n# If we have player data, do imitation learning first\nif has_player_data:\n    print(\"\\n=== Imitation Learning from Top Players ===\")\n    trainer.train_from_player_data('player_data.json', epochs=10)\n\n# Then self-play\nprint(\"\\n=== Self-Play Training ===\")\ntrained_model = trainer.train(num_episodes=2000, save_every=500)",
   "metadata": {
    "id": "run_training"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot training progress\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(trainer.episode_rewards)\n",
    "ax1.set_title('Average Episode Reward')\n",
    "ax1.set_xlabel('Episode (×100)')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(trainer.win_rates)\n",
    "ax2.set_title('Win Rate vs Self-Play Pool')\n",
    "ax2.set_xlabel('Episode (×100)')\n",
    "ax2.set_ylabel('Win Rate')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "plot"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save with timestamp so next run auto-finds it\nfrom datetime import datetime\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\nfilename = f'brain_{timestamp}.json'\n\nsave_brain_json(trained_model, filename)\nprint(f\"\\n✓ Saved as {filename}\")\nprint(\"Next run will auto-load this brain!\")\n\nif IN_COLAB:\n    files.download(filename)\n    print('Downloaded! Update index.html with this brain.')",
   "metadata": {
    "id": "save"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Next Steps\n\nAfter training completes, ask Iris to:\n1. Update `getVision()` in index.html for 60 inputs\n2. Replace the brain data with `brain_v2_final.json`\n\nThe new format adds:\n- Split food → health_pickup + gun_pickup\n- 4 state inputs at the end (health, position, threat, length)",
   "metadata": {
    "id": "howto"
   }
  }
 ]
}